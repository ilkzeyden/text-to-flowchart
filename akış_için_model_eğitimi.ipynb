{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7O2cCZU1sL8aQKvSnItj1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "qvSg_oHHywGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip uninstall torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --upgrade transformers datasets\n",
        "!pip cache purge"
      ],
      "metadata": {
        "id": "EVYjVv3Jt81B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h77v61gBynvr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "\n",
        "# 1. Excel'den verileri yükle\n",
        "file_path = r'/content/drive/My Drive/akış-diyagramı-örnekleri-verileri.xlsx'\n",
        "veriler = pd.read_excel(file_path)\n",
        "\n",
        "# Giriş ve çıkış verilerini ayıkla\n",
        "veriler['input_text'] = veriler['Akış Cümlesi']  # Giriş verisi\n",
        "veriler['output_text'] = veriler.apply(\n",
        "    lambda row: f\"{row['Adım No']} {row['Adım Açıklaması']} {row['Adım Türü']} {row['Koşul (Varsa)']} {row['Evet Durumu (Sonraki Adım)']} {row['Hayır Durumu (Sonraki Adım)']}\", axis=1\n",
        ")\n",
        "\n",
        "#veriler = veriler.rename(columns={\"Akış Cümlesi\": \"input_text\", \"Çıkış Verisi\": \"output_text\"})\n",
        "#veriler[\"output_text\"] = veriler[\"output_text\"].astype(str)  # Çıkışı metne dönüştür\n",
        "\n",
        "veriler[\"Evet Durumu (Sonraki Adım)\"] = veriler[\"Evet Durumu (Sonraki Adım)\"].apply(lambda x: None if isinstance(x, str) and x.strip() == '-' else x)\n",
        "veriler[\"Hayır Durumu (Sonraki Adım)\"] = veriler[\"Evet Durumu (Sonraki Adım)\"].apply(lambda x: None if isinstance(x, str) and x.strip() == '-' else x)\n",
        "\n",
        "# Veri önizleme\n",
        "print(\"Veri önizleme:\", veriler.head())\n",
        "\n",
        "# 2. Dataset formatına dönüştür\n",
        "dataset = Dataset.from_pandas(veriler)\n",
        "\n",
        "# 3. Model ve Tokenizer yükle\n",
        "model_name = \"t5-small\"  # Küçük bir model kullanıyoruz\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "#tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "# 4. Tokenizasyon işlemi\n",
        "def preprocess_function(examples):\n",
        "    inputs = [str(x) for x in examples[\"input_text\"]]  # Giriş cümleleri\n",
        "    targets = [str(x) for x in examples[\"output_text\"]]  # Çıkış detayları\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length').input_ids   # Çıkışlar daha uzun olabilir\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# 5. Eğitim ayarları\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    logging_dir=\"./logs\"\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Eğitim ve doğrulama verilerini ayırma\n",
        "\n",
        "train_dataset = tokenized_datasets.train_test_split(test_size=0.1, seed=42)[\"train\"]\n",
        "eval_dataset = tokenized_datasets.train_test_split(test_size=0.1, seed=42)[\"test\"]\n",
        "\n",
        "# 6. GPU kontrolü ve modele aktarım\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # Modeli GPU'ya taşı\n",
        "\n",
        "# 6. Eğitici (Trainer) tanımlama\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Doğrulama veri kümesini ekleyin\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# 6. Eğitici (Trainer) tanımlama\n",
        "#trainer = Trainer(\n",
        " #   model=model,\n",
        " #   args=training_args,\n",
        " #   train_dataset=tokenized_datasets,\n",
        " #   tokenizer=tokenizer\n",
        "#)\n",
        "\n",
        "# 7. Modeli eğit\n",
        "trainer.train()\n",
        "\n",
        "# 8. Tahmin testi\n",
        "def predict(cumle):\n",
        "    inputs = tokenizer(cumle, return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "    outputs = model.generate(**inputs, max_length=512)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test için örnek bir cümle\n",
        "test_cumle = \"Sipariş bilgilerini al, stokta ürün var mı kontrol et, varsa siparişi kaydet.\"\n",
        "tahmin = predict(test_cumle)\n",
        "print(\"Tahmin edilen adımlar ve türleri:\", tahmin)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Kayıpları izlemek için\n",
        "# Eğitim ve doğrulama kaybını kontrol et\n",
        "logs = trainer.state.log_history  # Eğitim loglarını al\n",
        "train_loss = [log['loss'] for log in logs if 'loss' in log]\n",
        "eval_loss = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
        "\n",
        "print(\"Eğitim Kayıpları (Train Loss):\", train_loss)\n",
        "print(\"Doğrulama Kayıpları (Eval Loss):\", eval_loss)\n"
      ],
      "metadata": {
        "id": "V7dx-Yz8KI6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Görsel analiz için\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Kayıpları çiz\n",
        "plt.plot(train_loss, label=\"Eğitim Kaybı\")\n",
        "plt.plot(eval_loss, label=\"Doğrulama Kaybı\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Kayıp (Loss)\")\n",
        "plt.title(\"Eğitim ve Doğrulama Kayıpları\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G-RRj8guKpyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}