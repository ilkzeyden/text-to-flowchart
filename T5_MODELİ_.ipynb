{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUZGo/u2+FhjDuCWspTGCL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sAEnylf7NxZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZNp5ZpFAOMDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT7Si45nNh7S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "#############################################\n",
        "# 1. Excel'den JSON'a DÃ¶nÃ¼ÅŸtÃ¼rme Fonksiyonu\n",
        "#############################################\n",
        "def excel_to_json(excel_file, json_file):\n",
        "    df = pd.read_excel(excel_file)\n",
        "    grouped_data = []\n",
        "\n",
        "    for akis_id, group in df.groupby(\"AkÄ±ÅŸ ID\"):\n",
        "        adÄ±mlar = []\n",
        "        for _, row in group.iterrows():\n",
        "            step_str = f'{row[\"AdÄ±m No\"]}: {row[\"AdÄ±m AÃ§Ä±klamasÄ±\"]}'\n",
        "            adÄ±mlar.append(step_str)\n",
        "\n",
        "        input_text = \"\\n\".join(adÄ±mlar)\n",
        "\n",
        "        output_steps = []\n",
        "        for _, row in group.iterrows():\n",
        "            step_str = f'{row[\"AdÄ±m No\"]}: {row[\"AdÄ±m TÃ¼rÃ¼\"]}: {row[\"AdÄ±m AÃ§Ä±klamasÄ±\"]}'\n",
        "            if str(row[\"AdÄ±m TÃ¼rÃ¼\"]).strip().lower() == \"koÅŸul\":\n",
        "                step_str += f' Yes={row[\"Evet Durumu (Sonraki AdÄ±m)\"]}, No={row[\"HayÄ±r Durumu (Sonraki AdÄ±m)\"]}'\n",
        "            else:\n",
        "                step_str += f' Next={row[\"Evet Durumu (Sonraki AdÄ±m)\"]}'\n",
        "            output_steps.append(step_str)\n",
        "\n",
        "        output_text = \"\\n\".join(output_steps)\n",
        "        grouped_data.append({\"input\": input_text, \"output\": output_text})\n",
        "\n",
        "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(grouped_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"EÄŸitim verileri '{json_file}' dosyasÄ±na kaydedildi.\")\n",
        "\n",
        "# Dosya yollarÄ±nÄ± belirle\n",
        "train_excel = r'/content/drive/My Drive/akÄ±ÅŸ-diyagramÄ±-Ã¶rnekleri-verileri.xlsx'\n",
        "test_excel  = r'/content/drive/My Drive/akÄ±ÅŸlar-test-verileri.xlsx'\n",
        "\n",
        "\n",
        "train_json = \"training_pairs.json\"\n",
        "test_json  = \"test_pairs.json\"\n",
        "\n",
        "# Excel verilerini JSON'a dÃ¶nÃ¼ÅŸtÃ¼r\n",
        "excel_to_json(train_excel, train_json)\n",
        "excel_to_json(test_excel, test_json)\n",
        "\n",
        "#############################################\n",
        "# 2. JSON'dan Dataset YÃ¼kleme\n",
        "#############################################\n",
        "def load_json_dataset(train_file, test_file):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        train_examples = json.load(f)\n",
        "    with open(test_file, 'r', encoding='utf-8') as f:\n",
        "        test_examples = json.load(f)\n",
        "    train_dataset = Dataset.from_list(train_examples)\n",
        "    test_dataset = Dataset.from_list(test_examples)\n",
        "    return DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "dataset = load_json_dataset(train_json, test_json)\n",
        "\n",
        "#############################################\n",
        "# 3. Model ve Tokenizer YÃ¼kleme\n",
        "#############################################\n",
        "custom_cache_dir = \"./huggingface_cache\"\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name, cache_dir=custom_cache_dir)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name, cache_dir=custom_cache_dir)\n",
        "\n",
        "#############################################\n",
        "# 4. Tokenizasyon & Preprocessing\n",
        "#############################################\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    targets = examples[\"output\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "#############################################\n",
        "# 5. EÄŸitim AyarlarÄ±\n",
        "#############################################\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5_flowchart_output\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=59,\n",
        "    save_total_limit=3,\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return {\"dummy_metric\": 0.0}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "#############################################\n",
        "# 6. EÄŸitimi BaÅŸlatma ve Test Ã–rneÄŸi Ãœzerinde Ã‡Ä±ktÄ± Alma\n",
        "#############################################\n",
        "if __name__ == \"__main__\":\n",
        "    trainer.train()\n",
        "\n",
        "    test_example = dataset[\"test\"][0][\"input\"]\n",
        "    input_ids = tokenizer.encode(test_example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids, max_length=512)\n",
        "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"ğŸ“ Girdi:\", test_example)\n",
        "    print(\"ğŸ¯ Ã‡Ä±ktÄ±:\")\n",
        "    print(predicted_text)\n",
        "\n",
        "    #############################################\n",
        "    # 7. Model Ã‡Ä±ktÄ±sÄ±nÄ± Excel DosyasÄ±na Kaydetme\n",
        "    #############################################\n",
        "\n",
        "    # Ã‡Ä±ktÄ±yÄ± Excel formatÄ±na uygun hale getir\n",
        "    lines = predicted_text.split(\"\\n\")\n",
        "    data = []\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.split(\": \")\n",
        "        if len(parts) >= 3:\n",
        "            step_no = parts[0]\n",
        "            step_type = parts[1]\n",
        "            step_description = \": \".join(parts[2:])  # AÃ§Ä±klama bazen \":\" iÃ§erebilir\n",
        "\n",
        "            # KoÅŸullu adÄ±mlar iÃ§in ayrÄ±ÅŸtÄ±rma\n",
        "            next_step = \"\"\n",
        "            yes_next = \"\"\n",
        "            no_next = \"\"\n",
        "\n",
        "            if \"Yes=\" in step_description or \"No=\" in step_description:\n",
        "                if \"Yes=\" in step_description:\n",
        "                    step_description, yes_next = step_description.split(\" Yes=\")\n",
        "                if \"No=\" in yes_next:\n",
        "                    yes_next, no_next = yes_next.split(\" No=\")\n",
        "            else:\n",
        "                if \"Next=\" in step_description:\n",
        "                    step_description, next_step = step_description.split(\" Next=\")\n",
        "\n",
        "            data.append([step_no, step_type, step_description, yes_next, no_next, next_step])\n",
        "\n",
        "    # DataFrame oluÅŸtur\n",
        "    df = pd.DataFrame(data, columns=[\"AdÄ±m No\", \"AdÄ±m TÃ¼rÃ¼\", \"AdÄ±m AÃ§Ä±klamasÄ±\", \"Evet Durumu (Sonraki AdÄ±m)\", \"HayÄ±r Durumu (Sonraki AdÄ±m)\", \"Sonraki AdÄ±m\"])\n",
        "\n",
        "    # Excel dosyasÄ±na kaydet\n",
        "    excel_output_path = \"flowchart_output.xlsx\"\n",
        "    df.to_excel(excel_output_path, index=False)\n",
        "\n",
        "    print(f\"âœ… Model Ã§Ä±ktÄ±sÄ± '{excel_output_path}' dosyasÄ±na baÅŸarÄ±yla kaydedildi!\")\n",
        "\n",
        "#abda9f461371669c2516207660e00058a83e1e09"
      ]
    }
  ]
}