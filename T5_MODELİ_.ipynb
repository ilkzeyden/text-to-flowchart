{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUZGo/u2+FhjDuCWspTGCL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sAEnylf7NxZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZNp5ZpFAOMDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT7Si45nNh7S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "#############################################\n",
        "# 1. Excel'den JSON'a Dönüştürme Fonksiyonu\n",
        "#############################################\n",
        "def excel_to_json(excel_file, json_file):\n",
        "    df = pd.read_excel(excel_file)\n",
        "    grouped_data = []\n",
        "\n",
        "    for akis_id, group in df.groupby(\"Akış ID\"):\n",
        "        adımlar = []\n",
        "        for _, row in group.iterrows():\n",
        "            step_str = f'{row[\"Adım No\"]}: {row[\"Adım Açıklaması\"]}'\n",
        "            adımlar.append(step_str)\n",
        "\n",
        "        input_text = \"\\n\".join(adımlar)\n",
        "\n",
        "        output_steps = []\n",
        "        for _, row in group.iterrows():\n",
        "            step_str = f'{row[\"Adım No\"]}: {row[\"Adım Türü\"]}: {row[\"Adım Açıklaması\"]}'\n",
        "            if str(row[\"Adım Türü\"]).strip().lower() == \"koşul\":\n",
        "                step_str += f' Yes={row[\"Evet Durumu (Sonraki Adım)\"]}, No={row[\"Hayır Durumu (Sonraki Adım)\"]}'\n",
        "            else:\n",
        "                step_str += f' Next={row[\"Evet Durumu (Sonraki Adım)\"]}'\n",
        "            output_steps.append(step_str)\n",
        "\n",
        "        output_text = \"\\n\".join(output_steps)\n",
        "        grouped_data.append({\"input\": input_text, \"output\": output_text})\n",
        "\n",
        "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(grouped_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Eğitim verileri '{json_file}' dosyasına kaydedildi.\")\n",
        "\n",
        "# Dosya yollarını belirle\n",
        "train_excel = r'/content/drive/My Drive/akış-diyagramı-örnekleri-verileri.xlsx'\n",
        "test_excel  = r'/content/drive/My Drive/akışlar-test-verileri.xlsx'\n",
        "\n",
        "\n",
        "train_json = \"training_pairs.json\"\n",
        "test_json  = \"test_pairs.json\"\n",
        "\n",
        "# Excel verilerini JSON'a dönüştür\n",
        "excel_to_json(train_excel, train_json)\n",
        "excel_to_json(test_excel, test_json)\n",
        "\n",
        "#############################################\n",
        "# 2. JSON'dan Dataset Yükleme\n",
        "#############################################\n",
        "def load_json_dataset(train_file, test_file):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        train_examples = json.load(f)\n",
        "    with open(test_file, 'r', encoding='utf-8') as f:\n",
        "        test_examples = json.load(f)\n",
        "    train_dataset = Dataset.from_list(train_examples)\n",
        "    test_dataset = Dataset.from_list(test_examples)\n",
        "    return DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "dataset = load_json_dataset(train_json, test_json)\n",
        "\n",
        "#############################################\n",
        "# 3. Model ve Tokenizer Yükleme\n",
        "#############################################\n",
        "custom_cache_dir = \"./huggingface_cache\"\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name, cache_dir=custom_cache_dir)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name, cache_dir=custom_cache_dir)\n",
        "\n",
        "#############################################\n",
        "# 4. Tokenizasyon & Preprocessing\n",
        "#############################################\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    targets = examples[\"output\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "#############################################\n",
        "# 5. Eğitim Ayarları\n",
        "#############################################\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5_flowchart_output\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=59,\n",
        "    save_total_limit=3,\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return {\"dummy_metric\": 0.0}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "#############################################\n",
        "# 6. Eğitimi Başlatma ve Test Örneği Üzerinde Çıktı Alma\n",
        "#############################################\n",
        "if __name__ == \"__main__\":\n",
        "    trainer.train()\n",
        "\n",
        "    test_example = dataset[\"test\"][0][\"input\"]\n",
        "    input_ids = tokenizer.encode(test_example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids, max_length=512)\n",
        "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"📝 Girdi:\", test_example)\n",
        "    print(\"🎯 Çıktı:\")\n",
        "    print(predicted_text)\n",
        "\n",
        "    #############################################\n",
        "    # 7. Model Çıktısını Excel Dosyasına Kaydetme\n",
        "    #############################################\n",
        "\n",
        "    # Çıktıyı Excel formatına uygun hale getir\n",
        "    lines = predicted_text.split(\"\\n\")\n",
        "    data = []\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.split(\": \")\n",
        "        if len(parts) >= 3:\n",
        "            step_no = parts[0]\n",
        "            step_type = parts[1]\n",
        "            step_description = \": \".join(parts[2:])  # Açıklama bazen \":\" içerebilir\n",
        "\n",
        "            # Koşullu adımlar için ayrıştırma\n",
        "            next_step = \"\"\n",
        "            yes_next = \"\"\n",
        "            no_next = \"\"\n",
        "\n",
        "            if \"Yes=\" in step_description or \"No=\" in step_description:\n",
        "                if \"Yes=\" in step_description:\n",
        "                    step_description, yes_next = step_description.split(\" Yes=\")\n",
        "                if \"No=\" in yes_next:\n",
        "                    yes_next, no_next = yes_next.split(\" No=\")\n",
        "            else:\n",
        "                if \"Next=\" in step_description:\n",
        "                    step_description, next_step = step_description.split(\" Next=\")\n",
        "\n",
        "            data.append([step_no, step_type, step_description, yes_next, no_next, next_step])\n",
        "\n",
        "    # DataFrame oluştur\n",
        "    df = pd.DataFrame(data, columns=[\"Adım No\", \"Adım Türü\", \"Adım Açıklaması\", \"Evet Durumu (Sonraki Adım)\", \"Hayır Durumu (Sonraki Adım)\", \"Sonraki Adım\"])\n",
        "\n",
        "    # Excel dosyasına kaydet\n",
        "    excel_output_path = \"flowchart_output.xlsx\"\n",
        "    df.to_excel(excel_output_path, index=False)\n",
        "\n",
        "    print(f\"✅ Model çıktısı '{excel_output_path}' dosyasına başarıyla kaydedildi!\")\n",
        "\n",
        "#abda9f461371669c2516207660e00058a83e1e09"
      ]
    }
  ]
}