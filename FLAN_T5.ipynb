{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 1. Gereken Paketleri Yükle\n",
        "# ==========================================================\n",
        "!pip install -q transformers datasets evaluate nltk accelerate\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    TrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "xmZbQ_34NpiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 2. Dosyayı Google Drive'dan Colab'a Kopyala\n",
        "# ==========================================================\n",
        "original_data_path = \"/content/drive/My Drive/akış 1000 veri.jsonl\"\n",
        "local_data_path = \"/content/akış_1000_veri.jsonl\"\n",
        "\n",
        "# Kopyalama işlemi\n",
        "try:\n",
        "    shutil.copy(original_data_path, local_data_path)\n",
        "    print(f\"{original_data_path} başarıyla kopyalandı.\")\n",
        "except Exception as e:\n",
        "    raise FileNotFoundError(f\"Dosya kopyalanamadı: {e}\")\n"
      ],
      "metadata": {
        "id": "eLHbs_uoNtdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 3. Veri Setini Yükle\n",
        "# ==========================================================\n",
        "from datasets import Dataset\n",
        "\n",
        "# JSONL dosyasını pandas ile oku\n",
        "import pandas as pd\n",
        "df = pd.read_json(local_data_path, lines=True)\n",
        "\n",
        "# Dataset'e dönüştür\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# %80 eğitim - %20 test bölmesi\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "print(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dvwq1tTMNvMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 4. Model ve Tokenizer\n",
        "# ==========================================================\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "OnfUYsDMN6Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.column_names)\n"
      ],
      "metadata": {
        "id": "QVM94S84OVuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 5. Tokenizasyon\n",
        "# ==========================================================\n",
        "def tokenize(example):\n",
        "    messages = example[\"messages\"]\n",
        "\n",
        "    # Tüm mesajları birleştir (system + user)\n",
        "    chat_input = \"\"\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] in [\"system\", \"user\"]:\n",
        "            chat_input += f\"{msg['role']}: {msg['content']}\\n\"\n",
        "\n",
        "    # Son assistant cevabı (etiket)\n",
        "    assistant_msg = next((msg[\"content\"] for msg in reversed(messages) if msg[\"role\"] == \"assistant\"), \"\")\n",
        "\n",
        "    # Tokenize et\n",
        "    inputs = tokenizer(\n",
        "        chat_input.strip(),\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            assistant_msg,\n",
        "            max_length=128,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "wXiybdsxN7EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "gOjHdwfRO4PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 6. Data Collator\n",
        "# ==========================================================\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "3VRp8omNN9Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install evaluate nltk datasets\n"
      ],
      "metadata": {
        "id": "BwZmMa-WO-gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 7. Metrikler\n",
        "# ==========================================================\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    bleu_result = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "\n",
        "    pred_tokens = [pred.split() for pred in decoded_preds]\n",
        "    label_tokens = [label.split() for label in decoded_labels]\n",
        "\n",
        "    precision = precision_metric.compute(predictions=pred_tokens, references=label_tokens, average=\"micro\")[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=pred_tokens, references=label_tokens, average=\"micro\")[\"recall\"]\n",
        "    f1 = f1_metric.compute(predictions=pred_tokens, references=label_tokens, average=\"micro\")[\"f1\"]\n",
        "\n",
        "    return {\n",
        "        \"rouge1\": round(rouge_result[\"rouge1\"] * 100, 2),\n",
        "        \"rougeL\": round(rouge_result[\"rougeL\"] * 100, 2),\n",
        "        \"bleu\": round(bleu_result[\"bleu\"] * 100, 2),\n",
        "        \"precision\": round(precision * 100, 2),\n",
        "        \"recall\": round(recall * 100, 2),\n",
        "        \"f1\": round(f1 * 100, 2),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "twIChyPTN-2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 8. Eğitim Ayarları\n",
        "# ==========================================================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./flan-t5-finetuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    push_to_hub=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "a1PSdOayOBSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(tokenize, batched=False)"
      ],
      "metadata": {
        "id": "-kA8tNAfPbqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "5Gb6AiwsPl1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CDiSwrgfbrTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 9. Trainer ve Eğitim\n",
        "# ==========================================================\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./flan-t5-finetuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    # processing_class=tokenizer,  # Gerekirse ekle, yoksa kaldır\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "#  wandb şifresi = abda9f461371669c2516207660e00058a83e1e09"
      ],
      "metadata": {
        "id": "ZLxIDlFAOBxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOjbmvSOOT65wCYZfICd37p"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}