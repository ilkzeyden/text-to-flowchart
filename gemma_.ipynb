{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJhEFUZVoE9j"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-chache-dir --no-deps git+https://github.com/unslothai/unsloth.github"
      ],
      "metadata": {
        "id": "PPWGJ2R1q0Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "ACJuvBSIq0G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit= True\n",
        "\n",
        "fourbit_models = [\"unsloth/mistral-7b-v0.3-bnb-4bit\", # New Mistral v3 2x faster!\n",
        "\n",
        " \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "\n",
        " \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "\n",
        " \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "\n",
        " \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "\n",
        " \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "\n",
        " \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "\n",
        " \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "\n",
        " \"unsloth/mistral-7b-bnb-4bit\",\n",
        "\n",
        " \"unsloth/gemma-7b-bnb-4bit\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "JvxImjC9q0ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "Mdr5NxBrHCkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model page: https://huggingface.co/google/gemma-3-4b-it\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/gemma-3-4b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"
      ],
      "metadata": {
        "id": "_-lh7obhHCkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ðŸ¤—"
      ],
      "metadata": {
        "id": "MfZyOTvOHCkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "LwT9eSAzHCkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model page: https://huggingface.co/google/gemma-2-9b-it\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/gemma-2-9b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"
      ],
      "metadata": {
        "id": "YmxYkTF8M4c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ðŸ¤—"
      ],
      "metadata": {
        "id": "5X5kMiLYM4c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-2-9b-it\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ],
      "metadata": {
        "id": "VrVj3yEYM4c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\")"
      ],
      "metadata": {
        "id": "yI6sbmcuM4c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"google/gemma-3-4b-it\"  # veya \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "JpyU_fr9q0BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "9EAJsMux2AT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Dosya yolu\n",
        "dosya= r'/content/drive/My Drive/akÄ±ÅŸ verileri.jsonl'\n",
        "\n",
        "# TÃ¼m veri listesi\n",
        "all_data = []\n",
        "\n",
        "with open(dosya, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        messages = item.get(\"messages\", [])\n",
        "\n",
        "        user_msg = next((m[\"content\"] for m in messages if m[\"role\"] == \"user\"), None)\n",
        "        assistant_msg = next((m[\"content\"] for m in messages if m[\"role\"] == \"assistant\"), None)\n",
        "\n",
        "        if user_msg and assistant_msg:\n",
        "            all_data.append({\n",
        "                \"input_text\": user_msg.strip(),\n",
        "                \"output_text\": assistant_msg.strip()\n",
        "            })\n",
        "\n",
        "# Veriyi karÄ±ÅŸtÄ±r ve bÃ¶l\n",
        "random.shuffle(all_data)\n",
        "split_index = int(len(all_data) * 0.8)\n",
        "\n",
        "train_data = all_data[:split_index]\n",
        "test_data = all_data[split_index:]\n",
        "\n",
        "# Ã–rnek Ã§Ä±ktÄ±lar\n",
        "print(f\" Toplam veri: {len(all_data)}\")\n",
        "print(f\" EÄŸitim verisi: {len(train_data)}\")\n",
        "print(f\" Test verisi: {len(test_data)}\")\n",
        "print(\"\\n Ã–rnek eÄŸitim verisi:\")\n",
        "print(json.dumps(train_data[0], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "wckqYmsbZ0hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"google/gemma-2b-it\"  # veya \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "\n",
        "# MesajlarÄ± oluÅŸturma\n",
        "messages = [\n",
        "  #  {\n",
        " # \"role\": \"system\",\n",
        "  #\"content\": \"Senin gÃ¶revin, verilen adÄ±mlarÄ± bir akÄ±ÅŸ diyagramÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmeye hazÄ±r hale getirmektir. Sana bir dizi adÄ±m verilecektir. Bu adÄ±mlar iÃ§in her adÄ±mÄ±n tÃ¼rÃ¼nÃ¼ (Ã¶rneÄŸin 'Process', 'Condition', 'Database', 'Message Box', 'Send Email', 'Start', 'Show Form', and 'Web Service'.) ve hangi adÄ±mla baÄŸlantÄ±lÄ± olduÄŸunu belirlemelisin. AkÄ±ÅŸ diyagramÄ±nÄ± oluÅŸturmak iÃ§in her adÄ±mÄ±n tÃ¼rÃ¼nÃ¼ ve baÄŸlantÄ±larÄ±nÄ± doÄŸru ÅŸekilde tanÄ±mlaman gerekiyor.\"\n",
        "#  \"content\": \"After determining the step type for each step (e.g., 'Process', 'Condition', 'Database', 'Message Box', 'Send Email', 'Start', 'Show Form', 'Web Service'), explain which step each one is linked to. For condition steps, specify how to proceed to the corresponding steps based on the 'Yes' or 'No' outcomes.\"\n",
        " # }\n",
        "{\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"Your task is to analyze the given steps and prepare them for a flowchart. Each step must be assigned a type (Process, Condition, Database, Message Box, Send Email, Start, Show Form, or Web Service). Also, determine which step comes next. If the step is of type 'Condition', specify both 'yes-next' and 'no-next' paths. \\n\\nYour output must strictly follow this format:\\n\\nStep [No]: [Type] - '[Description]' next:[Next Step No]\\n\\nor\\n\\nStep [No]: [Type] - '[Description]' yes-next:[Step No], no-next:[Step No]\\n\\nRespond only in this format with clear and accurate outputs.\"\n",
        "}\n",
        "\n",
        ",\n",
        "    {\"role\": \"user\", \"content\": \"1: MÃ¼ÅŸteri, sistem Ã¼zerinden sipariÅŸ oluÅŸturur.\\n2: SipariÅŸ bilgileri veritabanÄ±na kaydedilir.\\n3: ÃœrÃ¼n stokta var mÄ±? Evet ise Ã¼rÃ¼n hazÄ±rlanÄ±r ve kargoya verilir. HayÄ±r ise mÃ¼ÅŸteriye stokta olmadÄ±ÄŸÄ± bildirilir.\\n4: ÃœrÃ¼n hazÄ±rlanÄ±r ve kargoya teslim edilir.\\n5: HayÄ±r, stokta yok. MÃ¼ÅŸteriye Ã¼rÃ¼nÃ¼n stokta olmadÄ±ÄŸÄ±na dair e-posta gÃ¶nderilir.\\n6: SipariÅŸ mÃ¼ÅŸteriye teslim edilir ve sÃ¼reÃ§ tamamlanÄ±r.\"}\n",
        "]\n",
        "\n",
        "# MesajlarÄ± birleÅŸtirme (tokenizer yalnÄ±zca bir string bekliyor)\n",
        "full_message = \"\\n\".join([msg['content'] for msg in messages])\n",
        "\n",
        "# Tokenizer ile input_id'leri oluÅŸturma\n",
        "input_ids = tokenizer(full_message, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\")\n",
        "\n",
        "# TextStreamer'Ä± baÅŸlatma\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Model ile metin Ã¼retme\n",
        "output = model.generate(input_ids=input_ids,\n",
        "                        max_new_tokens=256,\n",
        "                        pad_token_id=tokenizer.eos_token_id,\n",
        "                        streamer=text_streamer)\n",
        "\n",
        "# SonuÃ§larÄ± yazdÄ±rma\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "QQE9re90qz28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"google/gemma-2b-it\"  # veya \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n"
      ],
      "metadata": {
        "id": "xvudpAwi6KgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model=model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n"
      ],
      "metadata": {
        "id": "5zOfahcN_7kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(\"test_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "EbB4Pwk3BUxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# EÄŸer train_data ve test_data hÃ¢lÃ¢ bellekteyse:\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "test_dataset = Dataset.from_list(test_data)\n"
      ],
      "metadata": {
        "id": "DLu2mBUOBXig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import to_sharegpt\n",
        "\n",
        "train_dataset = to_sharegpt(\n",
        "    train_dataset,\n",
        "    merged_prompt=\"[[\\nYour input is:\\n{input_text}]]\",\n",
        "    output_column_name=\"output_text\",\n",
        "    conversation_extension=3\n",
        ")\n",
        "\n",
        "test_dataset = to_sharegpt(\n",
        "    test_dataset,\n",
        "    merged_prompt=\"[[\\nYour input is:\\n{input_text}]]\",\n",
        "    output_column_name=\"output_text\",\n",
        "    conversation_extension=3\n",
        ")\n"
      ],
      "metadata": {
        "id": "5cjRD-AlBcC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "\n",
        "train_dataset = standardize_sharegpt(train_dataset)\n",
        "test_dataset = standardize_sharegpt(test_dataset)\n"
      ],
      "metadata": {
        "id": "naZU3gfjBfI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import apply_chat_template\n",
        "\n",
        "chat_template = \"\"\"Below are some instructions that describe some tasks.\n",
        "\n",
        "Write responses that appropriately complete each request.\n",
        "\n",
        "### Instruction:\n",
        "{INPUT}\n",
        "\n",
        "### Response:\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "train_dataset = apply_chat_template(\n",
        "    dataset=train_dataset,\n",
        "    tokenizer=tokenizer,  # Model tokenizer'Ä± burada gerekli\n",
        "    chat_template=chat_template,\n",
        "    # default_system_message=\"You are a helpful assistant\"  # Ä°stersen aÃ§\n",
        ")\n",
        "\n",
        "test_dataset = apply_chat_template(\n",
        "    dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    chat_template=chat_template,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZiiteCKwBiMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.column_names)\n"
      ],
      "metadata": {
        "id": "tYBeSg2cB_rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gemma-finetuned\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "TvWwJKqDCGYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats= trainer.train()"
      ],
      "metadata": {
        "id": "zu5P_PWACTZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# Modeli ve tokenizer'Ä± yÃ¼kleme\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# MesajlarÄ± oluÅŸturma\n",
        "messages = [\n",
        "  #  {\n",
        " # \"role\": \"system\",\n",
        "  #\"content\": \"Senin gÃ¶revin, verilen adÄ±mlarÄ± bir akÄ±ÅŸ diyagramÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmeye hazÄ±r hale getirmektir. Sana bir dizi adÄ±m verilecektir. Bu adÄ±mlar iÃ§in her adÄ±mÄ±n tÃ¼rÃ¼nÃ¼ (Ã¶rneÄŸin 'Process', 'Condition', 'Database', 'Message Box', 'Send Email', 'Start', 'Show Form', and 'Web Service'.) ve hangi adÄ±mla baÄŸlantÄ±lÄ± olduÄŸunu belirlemelisin. AkÄ±ÅŸ diyagramÄ±nÄ± oluÅŸturmak iÃ§in her adÄ±mÄ±n tÃ¼rÃ¼nÃ¼ ve baÄŸlantÄ±larÄ±nÄ± doÄŸru ÅŸekilde tanÄ±mlaman gerekiyor.\"\n",
        "#  \"content\": \"After determining the step type for each step (e.g., 'Process', 'Condition', 'Database', 'Message Box', 'Send Email', 'Start', 'Show Form', 'Web Service'), explain which step each one is linked to. For condition steps, specify how to proceed to the corresponding steps based on the 'Yes' or 'No' outcomes.\"\n",
        " # }\n",
        "{\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"Your task is to analyze the given steps and prepare them for a flowchart. Each step must be assigned a type (Process, Condition, Database, Message Box, Send Email, Start, Show Form, or Web Service). Also, determine which step comes next. If the step is of type 'Condition', specify both 'yes-next' and 'no-next' paths. \\n\\nYour output must strictly follow this format:\\n\\nStep [No]: [Type] - '[Description]' next:[Next Step No]\\n\\nor\\n\\nStep [No]: [Type] - '[Description]' yes-next:[Step No], no-next:[Step No]\\n\\nRespond only in this format with clear and accurate outputs.\"\n",
        "}\n",
        "\n",
        ",\n",
        "    {\"role\": \"user\", \"content\": \"1: MÃ¼ÅŸteri, sistem Ã¼zerinden sipariÅŸ oluÅŸturur.\\n2: SipariÅŸ bilgileri veritabanÄ±na kaydedilir.\\n3: ÃœrÃ¼n stokta var mÄ±? Evet ise Ã¼rÃ¼n hazÄ±rlanÄ±r ve kargoya verilir. HayÄ±r ise mÃ¼ÅŸteriye stokta olmadÄ±ÄŸÄ± bildirilir.\\n4: ÃœrÃ¼n hazÄ±rlanÄ±r ve kargoya teslim edilir.\\n5: HayÄ±r, stokta yok. MÃ¼ÅŸteriye Ã¼rÃ¼nÃ¼n stokta olmadÄ±ÄŸÄ±na dair e-posta gÃ¶nderilir.\\n6: SipariÅŸ mÃ¼ÅŸteriye teslim edilir ve sÃ¼reÃ§ tamamlanÄ±r.\"}\n",
        "]\n",
        "\n",
        "\n",
        "# MesajlarÄ± birleÅŸtirme (tokenizer yalnÄ±zca bir string bekliyor)\n",
        "full_message = \"\\n\".join([msg['content'] for msg in messages])\n",
        "\n",
        "# Tokenizer ile input_id'leri oluÅŸturma\n",
        "input_ids = tokenizer(full_message, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\")\n",
        "\n",
        "# TextStreamer'Ä± baÅŸlatma\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Model ile metin Ã¼retme\n",
        "output = model.generate(input_ids=input_ids,\n",
        "                        max_new_tokens=256,\n",
        "                        pad_token_id=tokenizer.eos_token_id,\n",
        "                        streamer=text_streamer)\n",
        "\n",
        "# SonuÃ§larÄ± yazdÄ±rma\n",
        "print(output)"
      ],
      "metadata": {
        "id": "01un1YSnCh17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install rouge_score # Install the missing package\n",
        "!pip install evaluate"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "YDpgwliMFZ6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "# from datasets import load_metric  # Replaced this line\n",
        "\n",
        "from evaluate import load # Added this line\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "bleu_metric = load(\"bleu\")\n",
        "rouge_metric = load(\"rouge\") # Now rouge_metric should load without error\n",
        "\n",
        "true_outputs = []\n",
        "pred_outputs = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for example in tqdm(test_data):\n",
        "    input_text = example[\"input_text\"]\n",
        "    expected_output = example[\"output_text\"]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    ).to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    predicted_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    true_outputs.append(expected_output.strip())\n",
        "    pred_outputs.append(predicted_output)\n",
        "\n",
        "# Exact Match (tam eÅŸleÅŸme)\n",
        "exact_matches = [int(p == t) for p, t in zip(pred_outputs, true_outputs)]\n",
        "accuracy = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "# ROUGE ve BLEU hesapla\n",
        "bleu_score = bleu_metric.compute(predictions=pred_outputs, references=[[ref] for ref in true_outputs])\n",
        "rouge_score = rouge_metric.compute(predictions=pred_outputs, references=true_outputs)\n",
        "\n",
        "#f1 skoru hesaplama\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def compute_f1(pred, ref):\n",
        "    pred_tokens = pred.lower().split()\n",
        "    ref_tokens = ref.lower().split()\n",
        "\n",
        "    common = set(pred_tokens) & set(ref_tokens)\n",
        "\n",
        "    if not common:\n",
        "        return 0.0\n",
        "\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(ref_tokens)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return round(f1, 4)\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "dosya= r'/content/drive/My Drive/akÄ±ÅŸ verileri.jsonl'\n",
        "\n",
        "\n",
        "with open(dosya, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        messages = data[\"messages\"]\n",
        "\n",
        "        # GerÃ§ek cevap ve model tahmini aynÄ±ysa devam et\n",
        "        if len(messages) >= 3:\n",
        "            user_input = messages[1][\"content\"]\n",
        "            expected_output = messages[2][\"content\"]  # assistant cevabÄ± (ground truth)\n",
        "            prediction = messages[2][\"content\"]       # Åžimdilik tahmin de aynÄ± yerden\n",
        "\n",
        "            f1 = compute_f1(prediction, expected_output)\n",
        "            f1_scores.append(f1)\n",
        "\n",
        "if f1_scores:\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "    print(f\" Ortalama F1 Skoru: {avg_f1:.4f}\")\n",
        "else:\n",
        "    print(\" Uygun veri bulunamadÄ±, F1 skoru hesaplanamadÄ±.\")\n",
        "\n",
        "print(\" Exact Match Accuracy:\", round(accuracy, 4))\n",
        "print(\" BLEU:\", round(bleu_score[\"bleu\"], 4))\n",
        "print(\" ROUGE:\", {k: round(v, 4) for k, v in rouge_score.items()})\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rgaDaSFGFaGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"ðŸ”¹ Input:\", test_data[i][\"input_text\"])\n",
        "    print(\"âœ… Expected:\", true_outputs[i])\n",
        "    print(\"ðŸ¤– Predicted:\", pred_outputs[i])\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "nbkPu-Q8W6g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "yJdGJN_PmfwY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}