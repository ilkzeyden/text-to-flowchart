{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJhEFUZVoE9j"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-chache-dir --no-deps git+https://github.com/unslothai/unsloth.github"
      ],
      "metadata": {
        "id": "PPWGJ2R1q0Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "ACJuvBSIq0G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit= True\n",
        "\n",
        "fourbit_models = [\"unsloth/mistral-7b-v0.3-bnb-4bit\", # New Mistral v3 2x faster!\n",
        "\n",
        " \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "\n",
        " \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "\n",
        " \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "\n",
        " \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "\n",
        " \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "\n",
        " \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "\n",
        " \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "\n",
        " \"unsloth/mistral-7b-bnb-4bit\",\n",
        "\n",
        " \"unsloth/gemma-7b-bnb-4bit\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "JvxImjC9q0ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "Mdr5NxBrHCkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model page: https://huggingface.co/google/gemma-3-4b-it\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/gemma-3-4b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "_-lh7obhHCkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
      ],
      "metadata": {
        "id": "MfZyOTvOHCkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "LwT9eSAzHCkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model page: https://huggingface.co/google/gemma-2-9b-it\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/gemma-2-9b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "YmxYkTF8M4c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
      ],
      "metadata": {
        "id": "5X5kMiLYM4c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-2-9b-it\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ],
      "metadata": {
        "id": "VrVj3yEYM4c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\")"
      ],
      "metadata": {
        "id": "yI6sbmcuM4c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"google/gemma-3-4b-it\"  # veya \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "JpyU_fr9q0BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "9EAJsMux2AT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Dosya yolu\n",
        "dosya= r'/content/drive/My Drive/akƒ±≈ü verileri.jsonl'\n",
        "\n",
        "# T√ºm veri listesi\n",
        "all_data = []\n",
        "\n",
        "with open(dosya, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        messages = item.get(\"messages\", [])\n",
        "\n",
        "        user_msg = next((m[\"content\"] for m in messages if m[\"role\"] == \"user\"), None)\n",
        "        assistant_msg = next((m[\"content\"] for m in messages if m[\"role\"] == \"assistant\"), None)\n",
        "\n",
        "        if user_msg and assistant_msg:\n",
        "            all_data.append({\n",
        "                \"input_text\": user_msg.strip(),\n",
        "                \"output_text\": assistant_msg.strip()\n",
        "            })\n",
        "\n",
        "# Veriyi karƒ±≈ütƒ±r ve b√∂l\n",
        "random.shuffle(all_data)\n",
        "split_index = int(len(all_data) * 0.8)\n",
        "\n",
        "train_data = all_data[:split_index]\n",
        "test_data = all_data[split_index:]\n",
        "\n",
        "# √ñrnek √ßƒ±ktƒ±lar\n",
        "print(f\" Toplam veri: {len(all_data)}\")\n",
        "print(f\" Eƒüitim verisi: {len(train_data)}\")\n",
        "print(f\" Test verisi: {len(test_data)}\")\n",
        "print(\"\\n √ñrnek eƒüitim verisi:\")\n",
        "print(json.dumps(train_data[0], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "wckqYmsbZ0hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"google/gemma-2b-it\"  # veya \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "\n",
        "# Mesajlarƒ± olu≈üturma\n",
        "messages = [\n",
        "  #  {\n",
        " # \"role\": \"system\",\n",
        "  #\"content\": \"Senin g√∂revin, verilen adƒ±mlarƒ± bir akƒ±≈ü diyagramƒ±na d√∂n√º≈üt√ºrmeye hazƒ±r hale getirmektir. Sana bir dizi adƒ±m verilecektir. Bu adƒ±mlar i√ßin her adƒ±mƒ±n t√ºr√ºn√º (√∂rneƒüin 'Process', 'Condition', 'Database', 'Message Box', 'Send Email', 'Start', 'Show Form', and 'Web Service'.) ve hangi adƒ±mla baƒülantƒ±lƒ± olduƒüunu belirlemelisin. Akƒ±≈ü diyagramƒ±nƒ± olu≈üturmak i√ßin her adƒ±mƒ±n t√ºr√ºn√º ve baƒülantƒ±larƒ±nƒ± doƒüru ≈üekilde tanƒ±mlaman gerekiyor.\"\n",
        "#  \"content\": \"After determining the step type for each step (e.g., 'Process', 'Condition', 'Database', 'Message Box', 'Send Email', 'Start', 'Show Form', 'Web Service'), explain which step each one is linked to. For condition steps, specify how to proceed to the corresponding steps based on the 'Yes' or 'No' outcomes.\"\n",
        " # }\n",
        "{\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"Your task is to analyze the given steps and prepare them for a flowchart. Each step must be assigned a type (Process, Condition, Database, Message Box, Send Email, Start, Show Form, or Web Service). Also, determine which step comes next. If the step is of type 'Condition', specify both 'yes-next' and 'no-next' paths. \\n\\nYour output must strictly follow this format:\\n\\nStep [No]: [Type] - '[Description]' next:[Next Step No]\\n\\nor\\n\\nStep [No]: [Type] - '[Description]' yes-next:[Step No], no-next:[Step No]\\n\\nRespond only in this format with clear and accurate outputs.\"\n",
        "}\n",
        "\n",
        ",\n",
        "    {\"role\": \"user\", \"content\": \"1: M√º≈üteri, sistem √ºzerinden sipari≈ü olu≈üturur.\\n2: Sipari≈ü bilgileri veritabanƒ±na kaydedilir.\\n3: √úr√ºn stokta var mƒ±? Evet ise √ºr√ºn hazƒ±rlanƒ±r ve kargoya verilir. Hayƒ±r ise m√º≈üteriye stokta olmadƒ±ƒüƒ± bildirilir.\\n4: √úr√ºn hazƒ±rlanƒ±r ve kargoya teslim edilir.\\n5: Hayƒ±r, stokta yok. M√º≈üteriye √ºr√ºn√ºn stokta olmadƒ±ƒüƒ±na dair e-posta g√∂nderilir.\\n6: Sipari≈ü m√º≈üteriye teslim edilir ve s√ºre√ß tamamlanƒ±r.\"}\n",
        "]\n",
        "\n",
        "# Mesajlarƒ± birle≈ütirme (tokenizer yalnƒ±zca bir string bekliyor)\n",
        "full_message = \"\\n\".join([msg['content'] for msg in messages])\n",
        "\n",
        "# Tokenizer ile input_id'leri olu≈üturma\n",
        "input_ids = tokenizer(full_message, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\")\n",
        "\n",
        "# TextStreamer'ƒ± ba≈ülatma\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Model ile metin √ºretme\n",
        "output = model.generate(input_ids=input_ids,\n",
        "                        max_new_tokens=256,\n",
        "                        pad_token_id=tokenizer.eos_token_id,\n",
        "                        streamer=text_streamer)\n",
        "\n",
        "# Sonu√ßlarƒ± yazdƒ±rma\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "QQE9re90qz28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"google/gemma-2b-it\"  # veya \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n"
      ],
      "metadata": {
        "id": "xvudpAwi6KgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model=model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n"
      ],
      "metadata": {
        "id": "5zOfahcN_7kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(\"test_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "EbB4Pwk3BUxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Eƒüer train_data ve test_data h√¢l√¢ bellekteyse:\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "test_dataset = Dataset.from_list(test_data)\n"
      ],
      "metadata": {
        "id": "DLu2mBUOBXig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import to_sharegpt\n",
        "\n",
        "train_dataset = to_sharegpt(\n",
        "    train_dataset,\n",
        "    merged_prompt=\"[[\\nYour input is:\\n{input_text}]]\",\n",
        "    output_column_name=\"output_text\",\n",
        "    conversation_extension=3\n",
        ")\n",
        "\n",
        "test_dataset = to_sharegpt(\n",
        "    test_dataset,\n",
        "    merged_prompt=\"[[\\nYour input is:\\n{input_text}]]\",\n",
        "    output_column_name=\"output_text\",\n",
        "    conversation_extension=3\n",
        ")\n"
      ],
      "metadata": {
        "id": "5cjRD-AlBcC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "\n",
        "train_dataset = standardize_sharegpt(train_dataset)\n",
        "test_dataset = standardize_sharegpt(test_dataset)\n"
      ],
      "metadata": {
        "id": "naZU3gfjBfI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import apply_chat_template\n",
        "\n",
        "chat_template = \"\"\"Below are some instructions that describe some tasks.\n",
        "\n",
        "Write responses that appropriately complete each request.\n",
        "\n",
        "### Instruction:\n",
        "{INPUT}\n",
        "\n",
        "### Response:\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "train_dataset = apply_chat_template(\n",
        "    dataset=train_dataset,\n",
        "    tokenizer=tokenizer,  # Model tokenizer'ƒ± burada gerekli\n",
        "    chat_template=chat_template,\n",
        "    # default_system_message=\"You are a helpful assistant\"  # ƒ∞stersen a√ß\n",
        ")\n",
        "\n",
        "test_dataset = apply_chat_template(\n",
        "    dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    chat_template=chat_template,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZiiteCKwBiMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.column_names)\n"
      ],
      "metadata": {
        "id": "tYBeSg2cB_rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gemma-finetuned\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "TvWwJKqDCGYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats= trainer.train()"
      ],
      "metadata": {
        "id": "zu5P_PWACTZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# Modeli ve tokenizer'ƒ± y√ºkleme\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Mesajlarƒ± olu≈üturma\n",
        "messages = [\n",
        "  #  {\n",
        " # \"role\": \"system\",\n",
        "  #\"content\": \"Senin g√∂revin, verilen adƒ±mlarƒ± bir akƒ±≈ü diyagramƒ±na d√∂n√º≈üt√ºrmeye hazƒ±r hale getirmektir. Sana bir dizi adƒ±m verilecektir. Bu adƒ±mlar i√ßin her adƒ±mƒ±n t√ºr√ºn√º (√∂rneƒüin 'Process', 'Condition', 'Database', 'Message Box', 'Send Email', 'Start', 'Show Form', and 'Web Service'.) ve hangi adƒ±mla baƒülantƒ±lƒ± olduƒüunu belirlemelisin. Akƒ±≈ü diyagramƒ±nƒ± olu≈üturmak i√ßin her adƒ±mƒ±n t√ºr√ºn√º ve baƒülantƒ±larƒ±nƒ± doƒüru ≈üekilde tanƒ±mlaman gerekiyor.\"\n",
        "#  \"content\": \"After determining the step type for each step (e.g., 'Process', 'Condition', 'Database', 'Message Box', 'Send Email', 'Start', 'Show Form', 'Web Service'), explain which step each one is linked to. For condition steps, specify how to proceed to the corresponding steps based on the 'Yes' or 'No' outcomes.\"\n",
        " # }\n",
        "{\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"Your task is to analyze the given steps and prepare them for a flowchart. Each step must be assigned a type (Process, Condition, Database, Message Box, Send Email, Start, Show Form, or Web Service). Also, determine which step comes next. If the step is of type 'Condition', specify both 'yes-next' and 'no-next' paths. \\n\\nYour output must strictly follow this format:\\n\\nStep [No]: [Type] - '[Description]' next:[Next Step No]\\n\\nor\\n\\nStep [No]: [Type] - '[Description]' yes-next:[Step No], no-next:[Step No]\\n\\nRespond only in this format with clear and accurate outputs.\"\n",
        "}\n",
        "\n",
        ",\n",
        "    {\"role\": \"user\", \"content\": \"1: M√º≈üteri, sistem √ºzerinden sipari≈ü olu≈üturur.\\n2: Sipari≈ü bilgileri veritabanƒ±na kaydedilir.\\n3: √úr√ºn stokta var mƒ±? Evet ise √ºr√ºn hazƒ±rlanƒ±r ve kargoya verilir. Hayƒ±r ise m√º≈üteriye stokta olmadƒ±ƒüƒ± bildirilir.\\n4: √úr√ºn hazƒ±rlanƒ±r ve kargoya teslim edilir.\\n5: Hayƒ±r, stokta yok. M√º≈üteriye √ºr√ºn√ºn stokta olmadƒ±ƒüƒ±na dair e-posta g√∂nderilir.\\n6: Sipari≈ü m√º≈üteriye teslim edilir ve s√ºre√ß tamamlanƒ±r.\"}\n",
        "]\n",
        "\n",
        "\n",
        "# Mesajlarƒ± birle≈ütirme (tokenizer yalnƒ±zca bir string bekliyor)\n",
        "full_message = \"\\n\".join([msg['content'] for msg in messages])\n",
        "\n",
        "# Tokenizer ile input_id'leri olu≈üturma\n",
        "input_ids = tokenizer(full_message, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\")\n",
        "\n",
        "# TextStreamer'ƒ± ba≈ülatma\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Model ile metin √ºretme\n",
        "output = model.generate(input_ids=input_ids,\n",
        "                        max_new_tokens=256,\n",
        "                        pad_token_id=tokenizer.eos_token_id,\n",
        "                        streamer=text_streamer)\n",
        "\n",
        "# Sonu√ßlarƒ± yazdƒ±rma\n",
        "print(output)"
      ],
      "metadata": {
        "id": "01un1YSnCh17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install rouge_score # Install the missing package\n",
        "!pip install evaluate"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "YDpgwliMFZ6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "# from datasets import load_metric  # Replaced this line\n",
        "\n",
        "from evaluate import load # Added this line\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "bleu_metric = load(\"bleu\")\n",
        "rouge_metric = load(\"rouge\") # Now rouge_metric should load without error\n",
        "\n",
        "true_outputs = []\n",
        "pred_outputs = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for example in tqdm(test_data):\n",
        "    input_text = example[\"input_text\"]\n",
        "    expected_output = example[\"output_text\"]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    ).to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    predicted_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    true_outputs.append(expected_output.strip())\n",
        "    pred_outputs.append(predicted_output)\n",
        "\n",
        "# Exact Match (tam e≈üle≈üme)\n",
        "exact_matches = [int(p == t) for p, t in zip(pred_outputs, true_outputs)]\n",
        "accuracy = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "# ROUGE ve BLEU hesapla\n",
        "bleu_score = bleu_metric.compute(predictions=pred_outputs, references=[[ref] for ref in true_outputs])\n",
        "rouge_score = rouge_metric.compute(predictions=pred_outputs, references=true_outputs)\n",
        "\n",
        "#f1 skoru hesaplama\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def compute_f1(pred, ref):\n",
        "    pred_tokens = pred.lower().split()\n",
        "    ref_tokens = ref.lower().split()\n",
        "\n",
        "    common = set(pred_tokens) & set(ref_tokens)\n",
        "\n",
        "    if not common:\n",
        "        return 0.0\n",
        "\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(ref_tokens)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return round(f1, 4)\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "dosya= r'/content/drive/My Drive/akƒ±≈ü verileri.jsonl'\n",
        "\n",
        "\n",
        "with open(dosya, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        messages = data[\"messages\"]\n",
        "\n",
        "        # Ger√ßek cevap ve model tahmini aynƒ±ysa devam et\n",
        "        if len(messages) >= 3:\n",
        "            user_input = messages[1][\"content\"]\n",
        "            expected_output = messages[2][\"content\"]  # assistant cevabƒ± (ground truth)\n",
        "            prediction = messages[2][\"content\"]       # ≈ûimdilik tahmin de aynƒ± yerden\n",
        "\n",
        "            f1 = compute_f1(prediction, expected_output)\n",
        "            f1_scores.append(f1)\n",
        "\n",
        "if f1_scores:\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "    print(f\" Ortalama F1 Skoru: {avg_f1:.4f}\")\n",
        "else:\n",
        "    print(\" Uygun veri bulunamadƒ±, F1 skoru hesaplanamadƒ±.\")\n",
        "\n",
        "print(\" Exact Match Accuracy:\", round(accuracy, 4))\n",
        "print(\" BLEU:\", round(bleu_score[\"bleu\"], 4))\n",
        "print(\" ROUGE:\", {k: round(v, 4) for k, v in rouge_score.items()})\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rgaDaSFGFaGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"üîπ Input:\", test_data[i][\"input_text\"])\n",
        "    print(\"‚úÖ Expected:\", true_outputs[i])\n",
        "    print(\"ü§ñ Predicted:\", pred_outputs[i])\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "nbkPu-Q8W6g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "yJdGJN_PmfwY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}