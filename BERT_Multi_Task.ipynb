{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5Y4hSgxysHqwlMLfEumOL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "fIgp9nKHKkAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ISmpn1hKDTQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Adım: Verileri Yükleme\n",
        "# -----------------------------\n",
        "train_file = r'/content/drive/My Drive/akış-diyagramı-örnekleri-verileri.xlsx'\n",
        "valid_file = r'/content/drive/My Drive/akışlar-test-verileri.xlsx'\n",
        "\n",
        "df_train = pd.read_excel(train_file)\n",
        "df_valid = pd.read_excel(valid_file)\n",
        "\n",
        "# Adım Türüne göre Koşul Label'ı belirle (Koşul varsa 1, yoksa 0)\n",
        "df_train['Koşul Label'] = df_train['Adım Türü'].apply(lambda x: 1 if x == \"Koşul\" else 0)\n",
        "df_valid['Koşul Label'] = df_valid['Adım Türü'].apply(lambda x: 1 if x == \"Koşul\" else 0)\n",
        "\n",
        "# Boş olan Evet/Hayır durumlarını -1 yap (Eksik değerler için)\n",
        "df_train['Evet Durumu (Sonraki Adım)'] = pd.to_numeric(df_train['Evet Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "df_train['Hayır Durumu (Sonraki Adım)'] = pd.to_numeric(df_train['Hayır Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "df_valid['Evet Durumu (Sonraki Adım)'] = pd.to_numeric(df_valid['Evet Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "df_valid['Hayır Durumu (Sonraki Adım)'] = pd.to_numeric(df_valid['Hayır Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "# Eksik değerleri içermeyenleri filtrele\n",
        "df_train = df_train[(df_train['Evet Durumu (Sonraki Adım)'] != -1) & (df_train['Hayır Durumu (Sonraki Adım)'] != -1)]\n",
        "df_valid = df_valid[(df_valid['Evet Durumu (Sonraki Adım)'] != -1) & (df_valid['Hayır Durumu (Sonraki Adım)'] != -1)]\n",
        "\n",
        "# -----------------------------\n",
        "# 2. BERT Dataset Sınıfı\n",
        "# -----------------------------\n",
        "class BertMultiTaskDataset(Dataset):\n",
        "    def __init__(self, texts, labels_task1, labels_task2, labels_task3, labels_task4, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels_task1 = labels_task1\n",
        "        self.labels_task2 = labels_task2\n",
        "        self.labels_task3 = labels_task3\n",
        "        self.labels_task4 = labels_task4\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label_task1 = self.labels_task1[idx]\n",
        "        label_task2 = self.labels_task2[idx]\n",
        "        label_task3 = self.labels_task3[idx]\n",
        "        label_task4 = self.labels_task4[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label_task1': torch.tensor(label_task1, dtype=torch.long),\n",
        "            'label_task2': torch.tensor(label_task2, dtype=torch.float),\n",
        "            'label_task3': torch.tensor(label_task3, dtype=torch.long),\n",
        "            'label_task4': torch.tensor(label_task4, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Model Tanımı\n",
        "# -----------------------------\n",
        "class BertMultiTaskModel(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_labels_task1):\n",
        "        super(BertMultiTaskModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.task1_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task1)\n",
        "        self.task2_classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.task3_classifier = nn.Linear(self.bert.config.hidden_size, 10)\n",
        "        self.task4_classifier = nn.Linear(self.bert.config.hidden_size, 10)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        task1_logits = self.task1_classifier(pooled_output)\n",
        "        task2_logits = self.task2_classifier(pooled_output).squeeze(-1)\n",
        "        task3_logits = self.task3_classifier(pooled_output)\n",
        "        task4_logits = self.task4_classifier(pooled_output)\n",
        "\n",
        "        return task1_logits, task2_logits, task3_logits, task4_logits\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Model Hazırlığı\n",
        "# -----------------------------\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
        "num_labels_task1 = len(df_train['Adım Türü Label'].unique())\n",
        "\n",
        "train_dataset = BertMultiTaskDataset(df_train['Adım Açıklaması'].tolist(), df_train['Adım Türü Label'].values, df_train['Koşul Label'].values, df_train['Evet Durumu (Sonraki Adım)'].values, df_train['Hayır Durumu (Sonraki Adım)'].values, bert_tokenizer)\n",
        "valid_dataset = BertMultiTaskDataset(df_valid['Adım Açıklaması'].tolist(), df_valid['Adım Türü Label'].values, df_valid['Koşul Label'].values, df_valid['Evet Durumu (Sonraki Adım)'].values, df_valid['Hayır Durumu (Sonraki Adım)'].values, bert_tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Model Eğitimi\n",
        "# -----------------------------\n",
        "model = BertMultiTaskModel(\"dbmdz/bert-base-turkish-cased\", num_labels_task1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion_task1 = nn.CrossEntropyLoss()\n",
        "criterion_task2 = nn.BCEWithLogitsLoss()\n",
        "criterion_task3 = nn.CrossEntropyLoss()\n",
        "criterion_task4 = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 6\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        task1_logits, task2_logits, task3_logits, task4_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "        loss = criterion_task1(task1_logits, batch['label_task1']) + criterion_task2(task2_logits, batch['label_task2']) + criterion_task3(task3_logits, batch['label_task3']) + criterion_task4(task4_logits, batch['label_task4'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} tamamlandı!\")\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Modelin çıktılarındaki sınıf sayısını doğrula\n",
        "num_labels_task1 = df_train['Adım Türü Label'].nunique()\n",
        "num_labels_task3 = df_train['Evet Durumu (Sonraki Adım)'].nunique()\n",
        "num_labels_task4 = df_train['Hayır Durumu (Sonraki Adım)'].nunique()\n",
        "\n",
        "# Modelin tanımı güncellendi\n",
        "self.task1_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task1)\n",
        "self.task3_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task3)\n",
        "self.task4_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task4)\n",
        "\n",
        "# Loss fonksiyonları güncellendi\n",
        "criterion_task1 = nn.CrossEntropyLoss()\n",
        "criterion_task3 = nn.CrossEntropyLoss(ignore_index=-1)  # -1 değerlerini yok say\n",
        "criterion_task4 = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# Eğitim döngüsü içinde\n",
        "optimizer.zero_grad()\n",
        "task1_logits, task2_logits, task3_logits, task4_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "# Hata kaynağını kontrol et\n",
        "max_label_task1 = batch['label_task1'].max().item()\n",
        "max_label_task3 = batch['label_task3'].max().item()\n",
        "max_label_task4 = batch['label_task4'].max().item()\n",
        "\n",
        "if max_label_task1 >= num_labels_task1 or max_label_task3 >= num_labels_task3 or max_label_task4 >= num_labels_task4:\n",
        "    print(f\"UYARI: Hedef etiketlerden biri modelin sınıf sayısından büyük! \"\n",
        "          f\"Task1: {max_label_task1}/{num_labels_task1}, \"\n",
        "          f\"Task3: {max_label_task3}/{num_labels_task3}, \"\n",
        "          f\"Task4: {max_label_task4}/{num_labels_task4}\")\n",
        "\n",
        "# Güncellenmiş loss hesaplama\n",
        "loss = (\n",
        "    criterion_task1(task1_logits, batch['label_task1']) +\n",
        "    criterion_task2(task2_logits, batch['label_task2']) +\n",
        "    criterion_task3(task3_logits, batch['label_task3']) +\n",
        "    criterion_task4(task4_logits, batch['label_task4'])\n",
        ")\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Model Değerlendirme\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "predictions_task1, predictions_task2 = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "        task1_logits, task2_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        preds_task1 = torch.argmax(task1_logits, dim=1).tolist()\n",
        "        preds_task2 = (torch.sigmoid(task2_logits) > 0.5).int().tolist()\n",
        "\n",
        "        predictions_task1.extend(preds_task1)\n",
        "        predictions_task2.extend(preds_task2)\n",
        "\n",
        "print(\"Task 1 (Adım Türü) Accuracy:\", accuracy_score(bert_valid_y1, predictions_task1))\n",
        "print(\"Task 1 (Adım Türü) Classification Report:\")\n",
        "print(classification_report(bert_valid_y1, predictions_task1))\n",
        "\n",
        "print(\"Task 2 (Koşul) Accuracy:\", accuracy_score(bert_valid_y2, predictions_task2))\n",
        "print(\"Task 2 (Koşul) Classification Report:\")\n",
        "print(classification_report(bert_valid_y2, predictions_task2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Adım: Verileri Yükleme\n",
        "# -----------------------------\n",
        "train_file = r'/content/drive/My Drive/akış-diyagramı-örnekleri-verileri.xlsx'\n",
        "valid_file = r'/content/drive/My Drive/akışlar-test-verileri.xlsx'\n",
        "\n",
        "df_train = pd.read_excel(train_file)\n",
        "df_valid = pd.read_excel(valid_file)\n",
        "\n",
        "# Adım Türüne göre Koşul Label'ı belirle (Koşul varsa 1, yoksa 0)\n",
        "df_train['Koşul Label'] = df_train['Adım Türü'].apply(lambda x: 1 if x == \"Koşul\" else 0)\n",
        "df_valid['Koşul Label'] = df_valid['Adım Türü'].apply(lambda x: 1 if x == \"Koşul\" else 0)\n",
        "\n",
        "# Boş olan Evet/Hayır durumlarını -1 yap (Eksik değerler için)\n",
        "df_train['Evet Durumu (Sonraki Adım)'] = pd.to_numeric(df_train['Evet Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "df_train['Hayır Durumu (Sonraki Adım)'] = pd.to_numeric(df_train['Hayır Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "df_valid['Evet Durumu (Sonraki Adım)'] = pd.to_numeric(df_valid['Evet Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "df_valid['Hayır Durumu (Sonraki Adım)'] = pd.to_numeric(df_valid['Hayır Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "# Eksik değerleri içermeyenleri filtrele\n",
        "df_train = df_train[(df_train['Evet Durumu (Sonraki Adım)'] != -1) & (df_train['Hayır Durumu (Sonraki Adım)'] != -1)]\n",
        "df_valid = df_valid[(df_valid['Evet Durumu (Sonraki Adım)'] != -1) & (df_valid['Hayır Durumu (Sonraki Adım)'] != -1)]\n",
        "\n",
        "# -----------------------------\n",
        "# 2. BERT Dataset Sınıfı\n",
        "# -----------------------------\n",
        "class BertMultiTaskDataset(Dataset):\n",
        "    def __init__(self, texts, labels_task1, labels_task2, labels_task3, labels_task4, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels_task1 = labels_task1\n",
        "        self.labels_task2 = labels_task2\n",
        "        self.labels_task3 = labels_task3\n",
        "        self.labels_task4 = labels_task4\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label_task1 = self.labels_task1[idx]\n",
        "        label_task2 = self.labels_task2[idx]\n",
        "        label_task3 = self.labels_task3[idx]\n",
        "        label_task4 = self.labels_task4[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label_task1': torch.tensor(label_task1, dtype=torch.long),\n",
        "            'label_task2': torch.tensor(label_task2, dtype=torch.float),\n",
        "            'label_task3': torch.tensor(label_task3, dtype=torch.long),\n",
        "            'label_task4': torch.tensor(label_task4, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Model Tanımı\n",
        "# -----------------------------\n",
        "class BertMultiTaskModel(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_labels_task1, num_labels_task3, num_labels_task4):\n",
        "        super(BertMultiTaskModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.task1_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task1)\n",
        "        self.task2_classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.task3_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task3)\n",
        "        self.task4_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        task1_logits = self.task1_classifier(pooled_output)\n",
        "        task2_logits = self.task2_classifier(pooled_output).squeeze(-1)\n",
        "        task3_logits = self.task3_classifier(pooled_output)\n",
        "        task4_logits = self.task4_classifier(pooled_output)\n",
        "\n",
        "        return task1_logits, task2_logits, task3_logits, task4_logits\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Model Hazırlığı\n",
        "# -----------------------------\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
        "num_labels_task1 = len(df_train['Adım Türü Label'].unique())\n",
        "num_labels_task3 = len(df_train['Evet Durumu (Sonraki Adım)'].unique())\n",
        "num_labels_task4 = len(df_train['Hayır Durumu (Sonraki Adım)'].unique())\n",
        "\n",
        "train_dataset = BertMultiTaskDataset(df_train['Adım Açıklaması'].tolist(), df_train['Adım Türü Label'].values, df_train['Koşul Label'].values, df_train['Evet Durumu (Sonraki Adım)'].values, df_train['Hayır Durumu (Sonraki Adım)'].values, bert_tokenizer)\n",
        "valid_dataset = BertMultiTaskDataset(df_valid['Adım Açıklaması'].tolist(), df_valid['Adım Türü Label'].values, df_valid['Koşul Label'].values, df_valid['Evet Durumu (Sonraki Adım)'].values, df_valid['Hayır Durumu (Sonraki Adım)'].values, bert_tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Model Eğitimi\n",
        "# -----------------------------\n",
        "model = BertMultiTaskModel(\"dbmdz/bert-base-turkish-cased\", num_labels_task1, num_labels_task3, num_labels_task4)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion_task1 = nn.CrossEntropyLoss()\n",
        "criterion_task2 = nn.BCEWithLogitsLoss()\n",
        "criterion_task3 = nn.CrossEntropyLoss(ignore_index=-1)  # -1 değerlerini yok say\n",
        "criterion_task4 = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "num_epochs = 6\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        task1_logits, task2_logits, task3_logits, task4_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        loss = (\n",
        "            criterion_task1(task1_logits, batch['label_task1']) +\n",
        "            criterion_task2(task2_logits, batch['label_task2']) +\n",
        "            criterion_task3(task3_logits, batch['label_task3']) +\n",
        "            criterion_task4(task4_logits, batch['label_task4'])\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} tamamlandı!\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Model Değerlendirme\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "predictions_task1, predictions_task2, predictions_task3, predictions_task4 = [], [], [], []\n",
        "with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "        task1_logits, task2_logits, task3_logits, task4_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        preds_task1 = torch.argmax(task1_logits, dim=1).tolist()\n",
        "        preds_task2 = (torch.sigmoid(task2_logits) > 0.5).int().tolist()\n",
        "        preds_task3 = torch.argmax(task3_logits, dim=1).tolist()\n",
        "        preds_task4 = torch.argmax(task4_logits, dim=1).tolist()\n",
        "\n",
        "        predictions_task1.extend(preds_task1)\n",
        "        predictions_task2.extend(preds_task2)\n",
        "        predictions_task3.extend(preds_task3)\n",
        "        predictions_task4.extend(preds_task4)\n",
        "\n",
        "# Sonuçlar\n",
        "print(\"Task 1 (Adım Türü) Accuracy:\", accuracy_score(df_valid['Adım Türü Label'], predictions_task1))\n",
        "print(\"Task 1 (Adım Türü) Classification Report:\")\n",
        "print(classification_report(df_valid['Adım Türü Label'], predictions_task1))\n",
        "\n",
        "print(\"Task 2 (Koşul) Accuracy:\", accuracy_score(df_valid['Koşul Label'], predictions_task2))\n",
        "print(\"Task 2 (Koşul) Classification Report:\")\n",
        "print(classification_report(df_valid['Koşul Label'], predictions_task2))\n",
        "\n",
        "print(\"Task 3 (Evet Durumu) Accuracy:\", accuracy_score(df_valid['Evet Durumu (Sonraki Adım)'], predictions_task3))\n",
        "print(\"Task 3 (Evet Durumu) Classification Report:\")\n",
        "print(classification_report(df_valid['Evet Durumu (Sonraki Adım)'], predictions_task3))\n",
        "\n",
        "print(\"Task 4 (Hayır Durumu) Accuracy:\", accuracy_score(df_valid['Hayır Durumu (Sonraki Adım)'], predictions_task4))\n",
        "print(\"Task 4 (Hayır Durumu) Classification Report:\")\n",
        "print(classification_report(df_valid['Hayır Durumu (Sonraki Adım)'], predictions_task4))\n"
      ],
      "metadata": {
        "id": "A9xXfLq4TyBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Model Tanımlaması\n",
        "class BertMultiTaskModel(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_labels_task1, num_labels_task3, num_labels_task4):\n",
        "        super(BertMultiTaskModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.task1_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task1)\n",
        "        self.task2_classifier = nn.Linear(self.bert.config.hidden_size, 1)  # Koşul (binary classification)\n",
        "        self.task3_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task3)\n",
        "        self.task4_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        task1_logits = self.task1_classifier(pooled_output)\n",
        "        task2_logits = self.task2_classifier(pooled_output).squeeze(-1)\n",
        "        task3_logits = self.task3_classifier(pooled_output)\n",
        "        task4_logits = self.task4_classifier(pooled_output)\n",
        "\n",
        "        return task1_logits, task2_logits, task3_logits, task4_logits\n",
        "\n",
        "# 2. Dataset Tanımlaması\n",
        "class FlowDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.iloc[idx]['Adım Açıklaması']  # Adım açıklaması\n",
        "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        labels_task1 = torch.tensor(self.df.iloc[idx]['Adım Türü Label'], dtype=torch.long)\n",
        "        labels_task2 = torch.tensor(self.df.iloc[idx]['Koşul'], dtype=torch.float)\n",
        "        labels_task3 = torch.tensor(self.df.iloc[idx]['Evet Durumu (Sonraki Adım)'], dtype=torch.long)\n",
        "        labels_task4 = torch.tensor(self.df.iloc[idx]['Hayır Durumu (Sonraki Adım)'], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "            'label_task1': labels_task1,\n",
        "            'label_task2': labels_task2,\n",
        "            'label_task3': labels_task3,\n",
        "            'label_task4': labels_task4\n",
        "        }\n",
        "\n",
        "# 3. Kaybı Hesaplamak için Kullanılan Fonksiyonlar\n",
        "criterion_task1 = nn.CrossEntropyLoss()\n",
        "criterion_task2 = nn.BCEWithLogitsLoss()\n",
        "criterion_task3 = nn.CrossEntropyLoss(ignore_index=-1)  # Eksik değerler için yok sayma\n",
        "criterion_task4 = nn.CrossEntropyLoss(ignore_index=-1)  # Eksik değerler için yok sayma\n",
        "\n",
        "# 4. Eğitim Parametrelerini Ayarlama\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertMultiTaskModel(bert_model_name='bert-base-uncased', num_labels_task1=10, num_labels_task3=10, num_labels_task4=10)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "# 5. Veri Yükleyici\n",
        "train_df = pd.read_csv('train.csv')  # Eğitim verinizi buradan yükleyebilirsiniz\n",
        "valid_df = pd.read_csv('valid.csv')  # Doğrulama verinizi buradan yükleyebilirsiniz\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "max_length = 128  # Maximum length of tokens\n",
        "\n",
        "train_dataset = FlowDataset(train_df, tokenizer, max_length)\n",
        "valid_dataset = FlowDataset(valid_df, tokenizer, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# 6. Eğitim Döngüsü\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "\n",
        "        task1_logits, task2_logits, task3_logits, task4_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        loss = (\n",
        "            criterion_task1(task1_logits, batch['label_task1']) +\n",
        "            criterion_task2(task2_logits, batch['label_task2']) +\n",
        "            criterion_task3(task3_logits, batch['label_task3']) +\n",
        "            criterion_task4(task4_logits, batch['label_task4'])\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} tamamlandı!\")\n",
        "\n",
        "# 7. Modeli Değerlendirme\n",
        "model.eval()\n",
        "predictions_task1, predictions_task2 = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "\n",
        "        task1_logits, task2_logits, task3_logits, task4_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        preds_task1 = torch.argmax(task1_logits, dim=1).tolist()\n",
        "        preds_task2 = (torch.sigmoid(task2_logits) > 0.5).int().tolist()\n",
        "\n",
        "        predictions_task1.extend(preds_task1)\n",
        "        predictions_task2.extend(preds_task2)\n",
        "\n",
        "# Doğrulama sonuçları\n",
        "print(\"Task 1 (Adım Türü) Accuracy:\", accuracy_score(valid_df['Adım Türü Label'], predictions_task1))\n",
        "print(\"Task 1 (Adım Türü) Classification Report:\")\n",
        "print(classification_report(valid_df['Adım Türü Label'], predictions_task1))\n",
        "\n",
        "print(\"Task 2 (Koşul) Accuracy:\", accuracy_score(valid_df['Koşul'], predictions_task2))\n",
        "print(\"Task 2 (Koşul) Classification Report:\")\n",
        "print(classification_report(valid_df['Koşul'], predictions_task2))\n",
        "\n",
        "# 8. Etiket Uyumunu Kontrol Etme\n",
        "max_label_task1 = train_df['Adım Türü Label'].max()\n",
        "max_label_task3 = train_df['Evet Durumu (Sonraki Adım)'].max()\n",
        "max_label_task4 = train_df['Hayır Durumu (Sonraki Adım)'].max()\n",
        "\n",
        "num_labels_task1 = len(train_df['Adım Türü Label'].unique())\n",
        "num_labels_task3 = len(train_df['Evet Durumu (Sonraki Adım)'].unique())\n",
        "num_labels_task4 = len(train_df['Hayır Durumu (Sonraki Adım)'].unique())\n",
        "\n",
        "assert max_label_task1 < num_labels_task1, f\"Task1: Etiketler num_labels_task1'dan büyük! ({max_label_task1} / {num_labels_task1})\"\n",
        "assert max_label_task3 < num_labels_task3, f\"Task3: Etiketler num_labels_task3'dan büyük! ({max_label_task3} / {num_labels_task3})\"\n",
        "assert max_label_task4 < num_labels_task4, f\"Task4: Etiketler num_labels_task4'dan büyük! ({max_label_task4} / {num_labels_task4})\"\n"
      ],
      "metadata": {
        "id": "B64bKKFHWTIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Adım: Verileri Yükleme\n",
        "# -----------------------------\n",
        "train_file = r'/content/drive/My Drive/akış-diyagramı-örnekleri-verileri.xlsx'\n",
        "valid_file = r'/content/drive/My Drive/akışlar-test-verileri.xlsx'\n",
        "\n",
        "df_train = pd.read_excel(train_file)\n",
        "df_valid = pd.read_excel(valid_file)\n",
        "\n",
        "# Adım Türüne göre Koşul Label'ı belirle (Koşul varsa 1, yoksa 0)\n",
        "df_train['Koşul Label'] = df_train['Adım Türü'].apply(lambda x: 1 if x == \"Koşul\" else 0)\n",
        "df_valid['Koşul Label'] = df_valid['Adım Türü'].apply(lambda x: 1 if x == \"Koşul\" else 0)\n",
        "\n",
        "# Boş olan Evet/Hayır durumlarını -1 yap (Eksik değerler için)\n",
        "df_train['Evet Durumu (Sonraki Adım)'] = pd.to_numeric(df_train['Evet Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "df_train['Hayır Durumu (Sonraki Adım)'] = pd.to_numeric(df_train['Hayır Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "df_valid['Evet Durumu (Sonraki Adım)'] = pd.to_numeric(df_valid['Evet Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "df_valid['Hayır Durumu (Sonraki Adım)'] = pd.to_numeric(df_valid['Hayır Durumu (Sonraki Adım)'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "# Eksik değerleri içermeyenleri filtrele\n",
        "df_train = df_train[(df_train['Evet Durumu (Sonraki Adım)'] != -1) & (df_train['Hayır Durumu (Sonraki Adım)'] != -1)]\n",
        "df_valid = df_valid[(df_valid['Evet Durumu (Sonraki Adım)'] != -1) & (df_valid['Hayır Durumu (Sonraki Adım)'] != -1)]\n",
        "\n",
        "# -----------------------------\n",
        "# 2. BERT Dataset Sınıfı\n",
        "# -----------------------------\n",
        "class BertMultiTaskDataset(Dataset):\n",
        "    def __init__(self, texts, labels_task1, labels_task2, labels_task3, labels_task4, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels_task1 = labels_task1\n",
        "        self.labels_task2 = labels_task2\n",
        "        self.labels_task3 = labels_task3\n",
        "        self.labels_task4 = labels_task4\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label_task1 = self.labels_task1[idx]\n",
        "        label_task2 = self.labels_task2[idx]\n",
        "        label_task3 = self.labels_task3[idx]\n",
        "        label_task4 = self.labels_task4[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label_task1': torch.tensor(label_task1, dtype=torch.long),\n",
        "            'label_task2': torch.tensor(label_task2, dtype=torch.float),\n",
        "            'label_task3': torch.tensor(label_task3, dtype=torch.long),\n",
        "            'label_task4': torch.tensor(label_task4, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Model Tanımı\n",
        "# -----------------------------\n",
        "class BertMultiTaskModel(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_labels_task1, num_labels_task3, num_labels_task4):\n",
        "        super(BertMultiTaskModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.task1_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task1)\n",
        "        self.task2_classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.task3_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task3)\n",
        "        self.task4_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_task4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        task1_logits = self.task1_classifier(pooled_output)\n",
        "        task2_logits = self.task2_classifier(pooled_output).squeeze(-1)\n",
        "        task3_logits = self.task3_classifier(pooled_output)\n",
        "        task4_logits = self.task4_classifier(pooled_output)\n",
        "\n",
        "        return task1_logits, task2_logits, task3_logits, task4_logits\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Model Hazırlığı\n",
        "# -----------------------------\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
        "num_labels_task1 = len(df_train['Adım Türü Label'].unique())\n",
        "num_labels_task3 = len(df_train['Evet Durumu (Sonraki Adım)'].unique())\n",
        "num_labels_task4 = len(df_train['Hayır Durumu (Sonraki Adım)'].unique())\n",
        "\n",
        "train_dataset = BertMultiTaskDataset(df_train['Adım Açıklaması'].tolist(), df_train['Adım Türü Label'].values, df_train['Koşul Label'].values, df_train['Evet Durumu (Sonraki Adım)'].values, df_train['Hayır Durumu (Sonraki Adım)'].values, bert_tokenizer)\n",
        "valid_dataset = BertMultiTaskDataset(df_valid['Adım Açıklaması'].tolist(), df_valid['Adım Türü Label'].values, df_valid['Koşul Label'].values, df_valid['Evet Durumu (Sonraki Adım)'].values, df_valid['Hayır Durumu (Sonraki Adım)'].values, bert_tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Model Eğitimi\n",
        "# -----------------------------\n",
        "model = BertMultiTaskModel(\"dbmdz/bert-base-turkish-cased\", num_labels_task1, num_labels_task3, num_labels_task4)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion_task1 = nn.CrossEntropyLoss()\n",
        "criterion_task2 = nn.BCEWithLogitsLoss()\n",
        "criterion_task3 = nn.CrossEntropyLoss(ignore_index=-1)  # -1 değerlerini yok say\n",
        "criterion_task4 = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "num_epochs = 6\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        task1_logits, task2_logits, task3_logits, task4_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        loss = (\n",
        "            criterion_task1(task1_logits, batch['label_task1']) +\n",
        "            criterion_task2(task2_logits, batch['label_task2']) +\n",
        "            criterion_task3(task3_logits, batch['label_task3']) +\n",
        "            criterion_task4(task4_logits, batch['label_task4'])\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1} tamamlandı!\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Model Değerlendirmesi\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "predictions_task1, predictions_task2 = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "        task1_logits, task2_logits, task3_logits, task4_logits = model(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        preds_task1 = torch.argmax(task1_logits, dim=1).tolist()\n",
        "        preds_task2 = (torch.sigmoid(task2_logits) > 0.5).int().tolist()\n",
        "\n",
        "        predictions_task1.extend(preds_task1)\n",
        "        predictions_task2.extend(preds_task2)\n",
        "\n",
        "# Doğrulama sonuçları\n",
        "print(\"Task 1 (Adım Türü) Accuracy:\", accuracy_score(df_valid['Adım Türü Label'], predictions_task1))\n",
        "print(\"Task 1 (Adım Türü) Classification Report:\")\n",
        "print(classification_report(df_valid['Adım Türü Label'], predictions_task1))\n",
        "\n",
        "print(\"Task 2 (Koşul) Accuracy:\", accuracy_score(df_valid['Koşul Label'], predictions_task2))\n",
        "print(\"Task 2 (Koşul) Classification Report:\")\n",
        "print(classification_report(df_valid['Koşul Label'], predictions_task2))\n"
      ],
      "metadata": {
        "id": "ooK--88dX2C0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}