{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOtKJL0fktUz8Bnj80JRvoz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNEktTd1wf7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJhEFUZVoE9j"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-chache-dir --no-deps git+https://github.com/unslothai/unsloth.github"
      ],
      "metadata": {
        "id": "PPWGJ2R1q0Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "ACJuvBSIq0G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install trl accelerate"
      ],
      "metadata": {
        "id": "NT9YhStSwUzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zNEM-X4DVOZ7"
      },
      "outputs": [],
      "source": [
        "# MAKE SURE TO RUN THE INSTALLATION CELL (e.g., OP6yy890VYe3) FIRST!\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import TextStreamer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported, to_sharegpt, standardize_sharegpt, apply_chat_template\n",
        "from unsloth.lora import get_peft_model # Import get_peft_model from unsloth.lora\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'Ä± baÄŸla\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. KÃ¼tÃ¼phaneleri YÃ¼kle ve Modeli BaÅŸlat ---\n",
        "# Unsloth ve diÄŸer gerekli kÃ¼tÃ¼phaneleri yÃ¼kle\n",
        "!pip install unsloth\n",
        "!pip install trl accelerate"
      ],
      "metadata": {
        "id": "OP6yy890VYe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parametreleri\n",
        "max_seq_length = 2048 # Daha kÄ±sa sÃ¼reÃ§ler iÃ§in bu deÄŸeri dÃ¼ÅŸÃ¼nebilirsiniz (Ã¶rn. 512 veya 1024)\n",
        "dtype = None # Otomatik olarak ayarlanÄ±r (bfloat16 veya float16)\n",
        "load_in_4bit = True\n",
        "\n",
        "# Modeli ve tokenizer'Ä± yÃ¼kle\n",
        "# Llama-3-8b-Instruct modeli seÃ§ildi.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "4jX4D06SVYbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Veri YÃ¼kleme ve HazÄ±rlÄ±k ---\n",
        "\n",
        "# JSONL dosyasÄ±nÄ±n yolu\n",
        "dosya = r'/content/drive/My Drive/akÄ±ÅŸ 1000 veri.jsonl'\n",
        "\n",
        "# Veriyi yÃ¼kle ve ShareGPT formatÄ±na uygun hale getir\n",
        "all_data = []\n",
        "\n",
        "with open(dosya, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        # JSONL dosyanÄ±zÄ±n doÄŸrudan \"messages\" formatÄ±nda olduÄŸunu varsayÄ±yorum\n",
        "        # {\"messages\": [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "        if \"messages\" in item and isinstance(item[\"messages\"], list):\n",
        "            all_data.append(item)\n",
        "        else:\n",
        "            # EÄŸer dosyanÄ±z {\"input_text\": \"...\", \"output_text\": \"...\"} formatÄ±ndaysa,\n",
        "            # bunu messages formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmeniz gerekir. Ã–rnek:\n",
        "            user_msg = item.get(\"input_text\")\n",
        "            assistant_msg = item.get(\"output_text\")\n",
        "            if user_msg and assistant_msg:\n",
        "                # Burada bir 'system' mesajÄ± eklemek, modelin amacÄ±nÄ± daha iyi anlamasÄ±nÄ± saÄŸlar\n",
        "                system_instruction = \"GÃ¶revin, verilen adÄ±mlarÄ± analiz ederek bir akÄ±ÅŸ diyagramÄ± iÃ§in hazÄ±rlamaktÄ±r. Her adÄ±m bir tÃ¼rle eÅŸleÅŸtirilmelidir (SÃ¼reÃ§, KoÅŸul, VeritabanÄ±, Mesaj Kutusu, E-posta GÃ¶nder, BaÅŸlat, Form GÃ¶ster veya Web Servisi). AyrÄ±ca, her adÄ±mÄ±n hangi adÄ±mdan sonra geldiÄŸini de belirlemelisin. EÄŸer adÄ±m tÃ¼rÃ¼ 'KoÅŸul' ise, 'evet-sonraki' ve 'hayÄ±r-sonraki' yollarÄ±nÄ± aÃ§Ä±kÃ§a belirtmelisin.\\n\\nÃ‡Ä±ktÄ±n kesinlikle ÅŸu formatta olmalÄ±dÄ±r:\\n\\nAdÄ±m [No]: [TÃ¼r] - '[AÃ§Ä±klama]' sonraki:[Sonraki AdÄ±m No]\\n\\nveya\\n\\nAdÄ±m [No]: [TÃ¼r] - '[AÃ§Ä±klama]' evet-sonraki:[AdÄ±m No], hayÄ±r-sonraki:[AdÄ±m No]\\n\\nYalnÄ±zca bu formatta ve aÃ§Ä±k, doÄŸru Ã§Ä±ktÄ±lar Ã¼ret.\"\n",
        "                all_data.append({\n",
        "                    \"messages\": [\n",
        "                        {\"role\": \"system\", \"content\": system_instruction},\n",
        "                        {\"role\": \"user\", \"content\": user_msg.strip()},\n",
        "                        {\"role\": \"assistant\", \"content\": assistant_msg.strip()}\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "# Veriyi karÄ±ÅŸtÄ±r ve eÄŸitim/test olarak bÃ¶l\n",
        "random.shuffle(all_data)\n",
        "split_index = int(len(all_data) * 0.8)\n",
        "\n",
        "train_data_raw = all_data[:split_index]\n",
        "test_data_raw = all_data[split_index:]\n",
        "\n",
        "print(f\"âœ… Toplam veri: {len(all_data)}\")\n",
        "print(f\"ğŸ“š EÄŸitim verisi: {len(train_data_raw)}\")\n",
        "print(f\"ğŸ§ª Test verisi: {len(test_data_raw)}\")\n"
      ],
      "metadata": {
        "id": "2wv8CgFsVYYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Datasets formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\n",
        "train_dataset = Dataset.from_list(train_data_raw)\n",
        "test_dataset = Dataset.from_list(test_data_raw)\n",
        "\n",
        "# Unsloth'un ChatML template'ini uygula\n",
        "# Bu, modeli Llama-3'Ã¼n standart diyalog formatÄ±na hazÄ±rlar.\n",
        "# ArtÄ±k Ã¶zel bir 'chat_template' tanÄ±mlamÄ±yoruz Ã§Ã¼nkÃ¼ Unsloth bunu otomatik hallediyor.\n",
        "train_dataset = train_dataset.map(lambda x: {\"prompt\": tokenizer.apply_chat_template(x[\"messages\"], tokenize=False)}, num_proc=os.cpu_count(), desc=\"Applying chat template to train dataset\")\n",
        "test_dataset = test_dataset.map(lambda x: {\"prompt\": tokenizer.apply_chat_template(x[\"messages\"], tokenize=False)}, num_proc=os.cpu_count(), desc=\"Applying chat template to test dataset\")\n",
        "\n",
        "# SFTTrainer'Ä±n beklediÄŸi \"text\" sÃ¼tununu ayarla\n",
        "# apply_chat_template sonrasÄ± \"text\" sÃ¼tunu yerine artÄ±k \"prompt\" sÃ¼tununu kullanÄ±yoruz.\n",
        "# Bu satÄ±rlara gerek kalmadÄ± Ã§Ã¼nkÃ¼ map fonksiyonu doÄŸrudan \"prompt\" sÃ¼tununu oluÅŸturuyor.\n",
        "# train_dataset = train_dataset.rename_column(\"text\", \"prompt\") # 'text' sÃ¼tununu 'prompt' olarak yeniden adlandÄ±rabiliriz\n",
        "# test_dataset = test_dataset.rename_column(\"text\", \"prompt\")"
      ],
      "metadata": {
        "id": "-yMa60e8VYWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EÄŸer modelinize ek PEFT adaptÃ¶rleri eklemek isterseniz (Ã¶nerilir)\n",
        "model = get_peft_model(\n",
        "    model=model,\n",
        "    r=16, # LoRA rank, deneyebilirsiniz\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "id": "d_xfGd9RVYTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Modeli Fine-tune Et ---\n",
        "\n",
        "# EÄŸitim argÃ¼manlarÄ±\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2, # BelleÄŸe gÃ¶re ayarlayÄ±n\n",
        "    gradient_accumulation_steps=4, # per_device_train_batch_size * gradient_accumulation_steps = effective batch size\n",
        "    warmup_steps=30, # IsÄ±nma adÄ±mlarÄ±\n",
        "    num_train_epochs=5, # Veri setinizin bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne gÃ¶re ayarlayÄ±n, overfitting'i izleyin\n",
        "    learning_rate=2e-5, # Daha konservatif bir Ã¶ÄŸrenme oranÄ±, Llama-3 iÃ§in genellikle iyi\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    save_strategy=\"epoch\", # Her epoch sonunda kaydet\n",
        "    evaluation_strategy=\"epoch\", # Her epoch sonunda deÄŸerlendir\n",
        "    load_best_model_at_end=True, # En iyi modeli yÃ¼kle\n",
        "    metric_for_best_model=\"eval_loss\", # En iyi model iÃ§in kullanÄ±lacak metrik\n",
        "    report_to=\"none\", # WandB vs. kullanmÄ±yorsan \"none\"\n",
        ")\n",
        "\n",
        "# Trainer nesnesi\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset, # Test setini doÄŸrulama iÃ§in kullan\n",
        "    dataset_text_field=\"prompt\", # apply_chat_template sonrasÄ± \"text\" sÃ¼tununu kullanÄ±yoruz\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=os.cpu_count(),\n",
        "    packing=False, # KÄ±sa diziler iÃ§in True olabilir, ancak ÅŸu an False kalsÄ±n\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# EÄŸitimi baÅŸlat\n",
        "print(\"\\nğŸš€ EÄŸitime BaÅŸlanÄ±yor...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"âœ… EÄŸitim TamamlandÄ±!\")"
      ],
      "metadata": {
        "id": "AEZKV_OLWBva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Modeli Kaydet (Ä°steÄŸe BaÄŸlÄ±) ---\n",
        "# EÄŸitilmiÅŸ modeli daha sonra kullanmak Ã¼zere kaydet\n",
        "#trainer.save_model(\"/content/drive/My Drive/fine_tuned_flowchart_model\")\n",
        "#tokenizer.save_pretrained(\"/content/drive/My Drive/fine_tuned_flowchart_model\")\n",
        "#print(\"\\nâœ… Model ve Tokenizer Drive'a kaydedildi.\")\n",
        "\n",
        "# --- 5. Test ve Ã‡Ä±karÄ±m (Inference) ---\n",
        "print(\"\\n--- Test ve Ã‡Ä±karÄ±m ---\")\n",
        "\n",
        "# EÄŸitimden sonra yeni bir model yÃ¼klemeye gerek yok, eÄŸitilmiÅŸ 'model' objesini kullanÄ±yoruz.\n",
        "\n",
        "# Ã–rnek bir kullanÄ±cÄ± mesajÄ± oluÅŸtur\n",
        "# Bu sistem mesajÄ±, eÄŸitim verinizdeki sistem mesajÄ± ile aynÄ± olmalÄ±!\n",
        "system_message_for_inference = \"GÃ¶revin, verilen adÄ±mlarÄ± analiz ederek bir akÄ±ÅŸ diyagramÄ± iÃ§in hazÄ±rlamaktÄ±r. Her adÄ±m bir tÃ¼rle eÅŸleÅŸtirilmelidir (SÃ¼reÃ§, KoÅŸul, VeritabanÄ±, Mesaj Kutusu, E-posta GÃ¶nder, BaÅŸlat, Form GÃ¶ster veya Web Servisi). AyrÄ±ca, her adÄ±mÄ±n hangi adÄ±mdan sonra geldiÄŸini de belirlemelisin. EÄŸer adÄ±m tÃ¼rÃ¼ 'KoÅŸul' ise, 'evet-sonraki' ve 'hayÄ±r-sonraki' yollarÄ±nÄ± aÃ§Ä±kÃ§a belirtmelisin.\\n\\nÃ‡Ä±ktÄ±n kesinlikle ÅŸu formatta olmalÄ±dÄ±r:\\n\\nAdÄ±m [No]: [TÃ¼r] - '[AÃ§Ä±klama]' sonraki:[Sonraki AdÄ±m No]\\n\\nveya\\n\\nAdÄ±m [No]: [TÃ¼r] - '[AÃ§Ä±klama]' evet-sonraki:[AdÄ±m No], hayÄ±r-sonraki:[AdÄ±m No]\\n\\nYalnÄ±zca bu formatta ve aÃ§Ä±k, doÄŸru Ã§Ä±ktÄ±lar Ã¼ret.\"\n",
        "\n",
        "user_input_example = \"1: MÃ¼ÅŸteri, sistem Ã¼zerinden sipariÅŸ oluÅŸturur.\\n2: SipariÅŸ bilgileri veritabanÄ±na kaydedilir.\\n3: ÃœrÃ¼n stokta var mÄ±? Evet ise Ã¼rÃ¼n hazÄ±rlanÄ±r ve kargoya verilir. HayÄ±r ise mÃ¼ÅŸteriye stokta olmadÄ±ÄŸÄ± bildirilir.\\n4: ÃœrÃ¼n hazÄ±rlanÄ±r ve kargoya teslim edilir.\\n5: HayÄ±r, stokta yok. MÃ¼ÅŸteriye Ã¼rÃ¼nÃ¼n stokta olmadÄ±ÄŸÄ±na dair e-posta gÃ¶nderilir.\\n6: SipariÅŸ mÃ¼ÅŸteriye teslim edilir ve sÃ¼reÃ§ tamamlanÄ±r.\"\n",
        "\n",
        "messages_for_inference = [\n",
        "    {\"role\": \"system\", \"content\": system_message_for_inference},\n",
        "    {\"role\": \"user\", \"content\": user_input_example}\n",
        "]\n",
        "\n",
        "# Tokenize et ve generation prompt'unu ekle\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages_for_inference,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True, # Llama-3'Ã¼n Ã¼retmeye baÅŸlamasÄ± iÃ§in bu gerekli\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# TextStreamer ile Ã§Ä±ktÄ±yÄ± akÄ±t\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Model ile metin Ã¼ret\n",
        "print(\"\\nğŸ¤– Model Tahmini (Ã–rnek):\")\n",
        "_ = model.generate(input_ids=input_ids,\n",
        "                     max_new_tokens=256, # Ãœretilecek maksimum token sayÄ±sÄ±\n",
        "                     pad_token_id=tokenizer.eos_token_id,\n",
        "                     streamer=text_streamer)\n"
      ],
      "metadata": {
        "id": "55NdygtAWFYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Metrikleri Hesapla (Test Verisi Ãœzerinden) ---\n",
        "print(\"\\n--- Metrik Hesaplama ---\")\n",
        "\n",
        "# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle (Colab oturumunda bir kez Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r)\n",
        "# !pip install rouge_score\n",
        "# !pip install evaluate\n",
        "\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "# Metrikleri yÃ¼kle\n",
        "bleu_metric = load(\"bleu\")\n",
        "rouge_metric = load(\"rouge\")\n",
        "\n",
        "true_outputs = []\n",
        "pred_outputs = []\n",
        "\n",
        "# Test setindeki raw veriyi kullan\n",
        "for item in tqdm(test_data_raw, desc=\"Generating predictions for test set\"):\n",
        "    # Her bir Ã¶ÄŸenin mesajlar listesinden kullanÄ±cÄ± girdisini ve asistan Ã§Ä±ktÄ±sÄ±nÄ± al\n",
        "    user_msg = next((m[\"content\"] for m in item[\"messages\"] if m[\"role\"] == \"user\"), None)\n",
        "    expected_output = next((m[\"content\"] for m in item[\"messages\"] if m[\"role\"] == \"assistant\"), None)\n",
        "    system_msg = next((m[\"content\"] for m in item[\"messages\"] if m[\"role\"] == \"system\"), None) # Sisteminizi de alÄ±n\n",
        "\n",
        "    if user_msg and expected_output and system_msg:\n",
        "        inference_messages = [\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg}\n",
        "        ]\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            inference_messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad(): # Tahmin yaparken gradient hesaplamayÄ± kapat\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256, # Ãœretilecek maksimum token sayÄ±sÄ±\n",
        "                do_sample=False, # Daha deterministik Ã§Ä±ktÄ± iÃ§in False\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Prompt kÄ±smÄ±nÄ± atlayarak sadece modelin Ã¼rettiÄŸi cevabÄ± decode et\n",
        "        # generate fonksiyonu tÃ¼m input_ids'i dÃ¶ndÃ¼rÃ¼r, bu yÃ¼zden prompt uzunluÄŸunu Ã§Ä±karÄ±rÄ±z\n",
        "        predicted_output = tokenizer.decode(generated_ids[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True).strip()\n",
        "\n",
        "        true_outputs.append(expected_output.strip())\n",
        "        pred_outputs.append(predicted_output.strip())\n",
        "\n",
        "# EÄŸer tahmin ve gerÃ§ek Ã§Ä±ktÄ± listeleri boÅŸ deÄŸilse metrikleri hesapla\n",
        "if len(true_outputs) > 0 and len(pred_outputs) > 0:\n",
        "    # âœ… Exact Match Accuracy\n",
        "    exact_matches = [int(p == t) for p, t in zip(pred_outputs, true_outputs)]\n",
        "    exact_accuracy = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "    # âœ… Relaxed Accuracy (kÃ¼Ã§Ã¼k harfe Ã§evir, boÅŸluklarÄ± kaldÄ±r)\n",
        "    def relaxed_match(pred, ref):\n",
        "        return pred.lower().replace(\" \", \"\").replace(\"'\", \"\") == ref.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
        "\n",
        "    relaxed_matches = [int(relaxed_match(p, t)) for p, t in zip(pred_outputs, true_outputs)]\n",
        "    relaxed_accuracy = sum(relaxed_matches) / len(relaxed_matches)\n",
        "\n",
        "    # âœ… BLEU ve ROUGE hesapla\n",
        "    # BLEU tek referans iÃ§in Ã§ift liste bekler\n",
        "    bleu_score = bleu_metric.compute(predictions=pred_outputs, references=[[ref] for ref in true_outputs])\n",
        "    rouge_score = rouge_metric.compute(predictions=pred_outputs, references=true_outputs)\n",
        "\n",
        "    # âœ… F1 hesaplama (basit token bazlÄ±)\n",
        "    def compute_f1_token_based(pred, ref):\n",
        "        pred_tokens = pred.lower().split()\n",
        "        ref_tokens = ref.lower().split()\n",
        "\n",
        "        if not pred_tokens and not ref_tokens:\n",
        "            return 1.0 # Her ikisi de boÅŸsa mÃ¼kemmel eÅŸleÅŸme\n",
        "\n",
        "        common = len(set(pred_tokens) & set(ref_tokens))\n",
        "        if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
        "            return 0.0 # Birisi boÅŸsa, diÄŸerleri deÄŸilse eÅŸleÅŸme yok\n",
        "\n",
        "        precision = common / len(pred_tokens)\n",
        "        recall = common / len(ref_tokens)\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    f1_scores = [compute_f1_token_based(p, r) for p, r in zip(pred_outputs, true_outputs)]\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    # âœ… SonuÃ§larÄ± yazdÄ±r\n",
        "    print(f\"ğŸ” Ortalama Token BazlÄ± F1 Skoru: {avg_f1:.4f}\")\n",
        "    print(f\"âœ… Exact Match Accuracy: {exact_accuracy:.4f}\")\n",
        "    print(f\"ğŸ¯ Relaxed Accuracy: {relaxed_accuracy:.4f}\")\n",
        "    print(f\"ğŸ“˜ BLEU: {round(bleu_score['bleu'], 4)}\")\n",
        "    print(\"ğŸ“— ROUGE:\", {k: round(v, 4) for k, v in rouge_score.items()})\n"
      ],
      "metadata": {
        "id": "aOzq2fesWNBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Ã–rnek tahminleri gÃ¶ster\n",
        "    print(\"\\n--- Ã–rnek Tahminler ve GerÃ§ek Ã‡Ä±ktÄ±lar ---\")\n",
        "    for i in range(min(15, len(test_data_raw))): # Ä°lk 15 veya mevcut veri sayÄ±sÄ± kadar gÃ¶ster\n",
        "        user_msg = next((m[\"content\"] for m in test_data_raw[i][\"messages\"] if m[\"role\"] == \"user\"), \"N/A\")\n",
        "        expected_output = next((m[\"content\"] for m in test_data_raw[i][\"messages\"] if m[\"role\"] == \"assistant\"), \"N/A\")\n",
        "\n",
        "        print(f\"ğŸ”¹ Input: {user_msg}\")\n",
        "        print(f\"âœ… Expected: {expected_output}\")\n",
        "        print(f\"ğŸ¤– Predicted: {pred_outputs[i]}\")\n",
        "        print(\"---\")\n",
        "else:\n",
        "    print(\"Metrik hesaplamak iÃ§in yeterli test verisi bulunamadÄ± veya tahminler boÅŸ.\")\n"
      ],
      "metadata": {
        "id": "GZ8YNfRpWSNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def parse_flowchart_step(step_text):\n",
        "    \"\"\"\n",
        "    Verilen akÄ±ÅŸ diyagramÄ± adÄ±m metnini parse ederek yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir sÃ¶zlÃ¼k dÃ¶ndÃ¼rÃ¼r.\n",
        "    Format: AdÄ±m [No]: [TÃ¼r] - '[AÃ§Ä±klama]' next:[Sonraki AdÄ±m No]\n",
        "    veya: AdÄ±m [No]: [TÃ¼r] - '[AÃ§Ä±klama]' yes-next:[AdÄ±m No], no-next:[AdÄ±m No]\n",
        "    \"\"\"\n",
        "    parsed_data = {\n",
        "        \"num\": None,\n",
        "        \"type\": None,\n",
        "        \"description\": None,\n",
        "        \"next\": None,\n",
        "        \"yes_next\": None,\n",
        "        \"no_next\": None,\n",
        "    }\n",
        "\n",
        "    # AdÄ±m numarasÄ±nÄ± ve tÃ¼rÃ¼ yakala\n",
        "    match = re.match(r\"AdÄ±m (\\d+): (SÃ¼reÃ§|KoÅŸul|VeritabanÄ±|Mesaj Kutusu|E-posta GÃ¶nder|BaÅŸlat|Form GÃ¶ster|Web Servisi) - '(.*?)'\", step_text)\n",
        "    if match:\n",
        "        parsed_data[\"num\"] = match.group(1)\n",
        "        parsed_data[\"type\"] = match.group(2)\n",
        "        parsed_data[\"description\"] = match.group(3).strip()\n",
        "\n",
        "        # Sonraki adÄ±m baÄŸlantÄ±larÄ±nÄ± yakala\n",
        "        remaining_text = step_text[match.end():].strip()\n",
        "        if remaining_text.startswith(\",\"):\n",
        "            remaining_text = remaining_text[1:].strip() # BaÅŸtaki virgÃ¼lÃ¼ kaldÄ±r\n",
        "\n",
        "        # KoÅŸul durumu\n",
        "        if \"yes-next:\" in remaining_text and \"no-next:\" in remaining_text:\n",
        "            yes_match = re.search(r\"yes-next:(\\d+)\", remaining_text)\n",
        "            no_match = re.search(r\"no-next:(\\d+)\", remaining_text)\n",
        "            if yes_match and no_match:\n",
        "                parsed_data[\"yes_next\"] = yes_match.group(1)\n",
        "                parsed_data[\"no_next\"] = no_match.group(1)\n",
        "        # Tekli next durumu\n",
        "        elif \"next:\" in remaining_text:\n",
        "            next_match = re.search(r\"next:(\\d+|None)\", remaining_text)\n",
        "            if next_match:\n",
        "                parsed_data[\"next\"] = next_match.group(1)\n",
        "    return parsed_data\n",
        "\n",
        "def calculate_component_metrics(true_outputs, pred_outputs):\n",
        "    \"\"\"\n",
        "    Her bir bileÅŸen (adÄ±m numarasÄ±, tÃ¼r, sonraki adÄ±m) iÃ§in doÄŸruluk, F1, Precision, Recall hesaplar.\n",
        "    \"\"\"\n",
        "    num_matches = []\n",
        "    type_matches = []\n",
        "    next_matches = [] # next, yes_next, no_next iÃ§in genel eÅŸleÅŸme\n",
        "\n",
        "    # AÃ§Ä±klama metinleri iÃ§in ayrÄ± listeler (BLEU/ROUGE zaten bunlarÄ± kullanÄ±yor)\n",
        "    true_descriptions = []\n",
        "    pred_descriptions = []\n",
        "\n",
        "    # Token tabanlÄ± F1 iÃ§in\n",
        "    all_true_tokens = []\n",
        "    all_pred_tokens = []\n",
        "\n",
        "    for true_text, pred_text in zip(true_outputs, pred_outputs):\n",
        "        # Her bir Ã§ok satÄ±rlÄ± Ã§Ä±ktÄ±yÄ± ayrÄ± adÄ±mlara bÃ¶l\n",
        "        true_steps = true_text.split('\\n')\n",
        "        pred_steps = pred_text.split('\\n')\n",
        "\n",
        "        # AdÄ±m numaralarÄ±na gÃ¶re eÅŸleÅŸtirme yapabilmek iÃ§in sÃ¶zlÃ¼klere Ã§evir\n",
        "        true_parsed_map = {p[\"num\"]: p for p in [parse_flowchart_step(s) for s in true_steps] if p[\"num\"]}\n",
        "        pred_parsed_map = {p[\"num\"]: p for p in [parse_flowchart_step(s) for s in pred_steps] if p[\"num\"]}\n",
        "\n",
        "        for num, true_step_data in true_parsed_map.items():\n",
        "            pred_step_data = pred_parsed_map.get(num)\n",
        "\n",
        "            # 1. AdÄ±m NumarasÄ± DoÄŸruluÄŸu (Bu zaten eÅŸleÅŸme anahtarÄ±mÄ±z, ancak yine de kontrol edelim)\n",
        "            num_matches.append(1 if pred_step_data and pred_step_data[\"num\"] == true_step_data[\"num\"] else 0)\n",
        "\n",
        "            if pred_step_data: # EÄŸer bu adÄ±m iÃ§in bir tahmin varsa\n",
        "                # 2. AdÄ±m TÃ¼rÃ¼ DoÄŸruluÄŸu\n",
        "                type_matches.append(1 if pred_step_data[\"type\"] == true_step_data[\"type\"] else 0)\n",
        "\n",
        "                # 3. Sonraki AdÄ±m BaÄŸlantÄ± DoÄŸruluÄŸu\n",
        "                is_next_match = 0\n",
        "                if true_step_data[\"type\"] == \"KoÅŸul\":\n",
        "                    if (pred_step_data[\"yes_next\"] == true_step_data[\"yes_next\"] and\n",
        "                        pred_step_data[\"no_next\"] == true_step_data[\"no_next\"]):\n",
        "                        is_next_match = 1\n",
        "                else: # SÃ¼reÃ§, VeritabanÄ± vb.\n",
        "                    if pred_step_data[\"next\"] == true_step_data[\"next\"]:\n",
        "                        is_next_match = 1\n",
        "                next_matches.append(is_next_match)\n",
        "\n",
        "                # 4. AÃ§Ä±klama Metni Ä°Ã§in Liste HazÄ±rlÄ±ÄŸÄ± (Zaten BLEU/ROUGE'da kullanÄ±lÄ±yor)\n",
        "                true_descriptions.append(true_step_data[\"description\"])\n",
        "                pred_descriptions.append(pred_step_data[\"description\"])\n",
        "\n",
        "                # 5. Token TabanlÄ± F1 iÃ§in token listeleri\n",
        "                all_true_tokens.extend(str(true_step_data.get(\"type\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"description\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"next\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"yes_next\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"no_next\", \"\")).lower().split())\n",
        "                all_pred_tokens.extend(str(pred_step_data.get(\"type\", \"\")).lower().split() + \\\n",
        "                                      str(pred_step_data.get(\"description\", \"\")).lower().split() + \\\n",
        "                                      str(pred_step_data.get(\"next\", \"\")).lower().split() + \\\n",
        "                                      str(pred_step_data.get(\"yes_next\", \"\")).lower().split() + \\\n",
        "                                      str(pred_step_data.get(\"no_next\", \"\")).lower().split())\n",
        "            else: # EÄŸer tahmin edilen adÄ±m yoksa, hepsi yanlÄ±ÅŸ sayÄ±lÄ±r\n",
        "                type_matches.append(0)\n",
        "                next_matches.append(0)\n",
        "                # AÃ§Ä±klama ve token listelerine boÅŸ ekle (ya da ignore edebiliriz)\n",
        "                true_descriptions.append(true_step_data[\"description\"])\n",
        "                pred_descriptions.append(\"\") # Tahmin yoksa boÅŸ aÃ§Ä±klama\n",
        "\n",
        "                all_true_tokens.extend(str(true_step_data.get(\"type\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"description\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"next\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"yes_next\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"no_next\", \"\")).lower().split())\n",
        "                # Pred tokenlarÄ± boÅŸ kalÄ±r\n",
        "\n",
        "    metrics = {}\n",
        "    if num_matches:\n",
        "        metrics[\"step_num_accuracy\"] = np.mean(num_matches)\n",
        "    if type_matches:\n",
        "        metrics[\"step_type_accuracy\"] = np.mean(type_matches)\n",
        "    if next_matches:\n",
        "        metrics[\"next_link_accuracy\"] = np.mean(next_matches)\n",
        "\n",
        "    # TÃ¼m tokenlarÄ±n Ã¼zerinden Precision, Recall, F1 hesaplama\n",
        "    if all_true_tokens and all_pred_tokens:\n",
        "        # sklearn'Ä±n precision_recall_fscore_support'u kullanmak iÃ§in etiketleme yapmalÄ±yÄ±z\n",
        "        # Basit bir token tabanlÄ± F1 iÃ§in Counter kullanmak daha kolay olabilir\n",
        "        common_tokens = len(list((Counter(all_pred_tokens) & Counter(all_true_tokens)).elements()))\n",
        "\n",
        "        if len(all_pred_tokens) == 0:\n",
        "            precision = 0.0\n",
        "        else:\n",
        "            precision = common_tokens / len(all_pred_tokens)\n",
        "\n",
        "        if len(all_true_tokens) == 0:\n",
        "            recall = 0.0\n",
        "        else:\n",
        "            recall = common_tokens / len(all_true_tokens)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        metrics[\"token_precision\"] = precision\n",
        "        metrics[\"token_recall\"] = recall\n",
        "        metrics[\"token_f1\"] = f1\n",
        "\n",
        "    return metrics, true_descriptions, pred_descriptions\n",
        "\n",
        "# Not: BLEU ve ROUGE hesaplamalarÄ± iÃ§in true_descriptions ve pred_descriptions kullanÄ±lacaktÄ±r.\n",
        "# Kodunuzda bu zaten mevcuttur."
      ],
      "metadata": {
        "id": "sJPEFZnyX_5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0d3be00"
      },
      "source": [
        "# Install necessary libraries for SFTTrainer\n",
        "!pip install trl accelerate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}