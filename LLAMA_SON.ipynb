{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOtKJL0fktUz8Bnj80JRvoz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNEktTd1wf7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJhEFUZVoE9j"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-chache-dir --no-deps git+https://github.com/unslothai/unsloth.github"
      ],
      "metadata": {
        "id": "PPWGJ2R1q0Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "ACJuvBSIq0G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install trl accelerate"
      ],
      "metadata": {
        "id": "NT9YhStSwUzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zNEM-X4DVOZ7"
      },
      "outputs": [],
      "source": [
        "# MAKE SURE TO RUN THE INSTALLATION CELL (e.g., OP6yy890VYe3) FIRST!\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import TextStreamer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported, to_sharegpt, standardize_sharegpt, apply_chat_template\n",
        "from unsloth.lora import get_peft_model # Import get_peft_model from unsloth.lora\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'ı bağla\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Kütüphaneleri Yükle ve Modeli Başlat ---\n",
        "# Unsloth ve diğer gerekli kütüphaneleri yükle\n",
        "!pip install unsloth\n",
        "!pip install trl accelerate"
      ],
      "metadata": {
        "id": "OP6yy890VYe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parametreleri\n",
        "max_seq_length = 2048 # Daha kısa süreçler için bu değeri düşünebilirsiniz (örn. 512 veya 1024)\n",
        "dtype = None # Otomatik olarak ayarlanır (bfloat16 veya float16)\n",
        "load_in_4bit = True\n",
        "\n",
        "# Modeli ve tokenizer'ı yükle\n",
        "# Llama-3-8b-Instruct modeli seçildi.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "4jX4D06SVYbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Veri Yükleme ve Hazırlık ---\n",
        "\n",
        "# JSONL dosyasının yolu\n",
        "dosya = r'/content/drive/My Drive/akış 1000 veri.jsonl'\n",
        "\n",
        "# Veriyi yükle ve ShareGPT formatına uygun hale getir\n",
        "all_data = []\n",
        "\n",
        "with open(dosya, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        # JSONL dosyanızın doğrudan \"messages\" formatında olduğunu varsayıyorum\n",
        "        # {\"messages\": [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "        if \"messages\" in item and isinstance(item[\"messages\"], list):\n",
        "            all_data.append(item)\n",
        "        else:\n",
        "            # Eğer dosyanız {\"input_text\": \"...\", \"output_text\": \"...\"} formatındaysa,\n",
        "            # bunu messages formatına dönüştürmeniz gerekir. Örnek:\n",
        "            user_msg = item.get(\"input_text\")\n",
        "            assistant_msg = item.get(\"output_text\")\n",
        "            if user_msg and assistant_msg:\n",
        "                # Burada bir 'system' mesajı eklemek, modelin amacını daha iyi anlamasını sağlar\n",
        "                system_instruction = \"Görevin, verilen adımları analiz ederek bir akış diyagramı için hazırlamaktır. Her adım bir türle eşleştirilmelidir (Süreç, Koşul, Veritabanı, Mesaj Kutusu, E-posta Gönder, Başlat, Form Göster veya Web Servisi). Ayrıca, her adımın hangi adımdan sonra geldiğini de belirlemelisin. Eğer adım türü 'Koşul' ise, 'evet-sonraki' ve 'hayır-sonraki' yollarını açıkça belirtmelisin.\\n\\nÇıktın kesinlikle şu formatta olmalıdır:\\n\\nAdım [No]: [Tür] - '[Açıklama]' sonraki:[Sonraki Adım No]\\n\\nveya\\n\\nAdım [No]: [Tür] - '[Açıklama]' evet-sonraki:[Adım No], hayır-sonraki:[Adım No]\\n\\nYalnızca bu formatta ve açık, doğru çıktılar üret.\"\n",
        "                all_data.append({\n",
        "                    \"messages\": [\n",
        "                        {\"role\": \"system\", \"content\": system_instruction},\n",
        "                        {\"role\": \"user\", \"content\": user_msg.strip()},\n",
        "                        {\"role\": \"assistant\", \"content\": assistant_msg.strip()}\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "# Veriyi karıştır ve eğitim/test olarak böl\n",
        "random.shuffle(all_data)\n",
        "split_index = int(len(all_data) * 0.8)\n",
        "\n",
        "train_data_raw = all_data[:split_index]\n",
        "test_data_raw = all_data[split_index:]\n",
        "\n",
        "print(f\"✅ Toplam veri: {len(all_data)}\")\n",
        "print(f\"📚 Eğitim verisi: {len(train_data_raw)}\")\n",
        "print(f\"🧪 Test verisi: {len(test_data_raw)}\")\n"
      ],
      "metadata": {
        "id": "2wv8CgFsVYYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Datasets formatına dönüştür\n",
        "train_dataset = Dataset.from_list(train_data_raw)\n",
        "test_dataset = Dataset.from_list(test_data_raw)\n",
        "\n",
        "# Unsloth'un ChatML template'ini uygula\n",
        "# Bu, modeli Llama-3'ün standart diyalog formatına hazırlar.\n",
        "# Artık özel bir 'chat_template' tanımlamıyoruz çünkü Unsloth bunu otomatik hallediyor.\n",
        "train_dataset = train_dataset.map(lambda x: {\"prompt\": tokenizer.apply_chat_template(x[\"messages\"], tokenize=False)}, num_proc=os.cpu_count(), desc=\"Applying chat template to train dataset\")\n",
        "test_dataset = test_dataset.map(lambda x: {\"prompt\": tokenizer.apply_chat_template(x[\"messages\"], tokenize=False)}, num_proc=os.cpu_count(), desc=\"Applying chat template to test dataset\")\n",
        "\n",
        "# SFTTrainer'ın beklediği \"text\" sütununu ayarla\n",
        "# apply_chat_template sonrası \"text\" sütunu yerine artık \"prompt\" sütununu kullanıyoruz.\n",
        "# Bu satırlara gerek kalmadı çünkü map fonksiyonu doğrudan \"prompt\" sütununu oluşturuyor.\n",
        "# train_dataset = train_dataset.rename_column(\"text\", \"prompt\") # 'text' sütununu 'prompt' olarak yeniden adlandırabiliriz\n",
        "# test_dataset = test_dataset.rename_column(\"text\", \"prompt\")"
      ],
      "metadata": {
        "id": "-yMa60e8VYWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğer modelinize ek PEFT adaptörleri eklemek isterseniz (önerilir)\n",
        "model = get_peft_model(\n",
        "    model=model,\n",
        "    r=16, # LoRA rank, deneyebilirsiniz\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "id": "d_xfGd9RVYTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Modeli Fine-tune Et ---\n",
        "\n",
        "# Eğitim argümanları\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2, # Belleğe göre ayarlayın\n",
        "    gradient_accumulation_steps=4, # per_device_train_batch_size * gradient_accumulation_steps = effective batch size\n",
        "    warmup_steps=30, # Isınma adımları\n",
        "    num_train_epochs=5, # Veri setinizin büyüklüğüne göre ayarlayın, overfitting'i izleyin\n",
        "    learning_rate=2e-5, # Daha konservatif bir öğrenme oranı, Llama-3 için genellikle iyi\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    save_strategy=\"epoch\", # Her epoch sonunda kaydet\n",
        "    evaluation_strategy=\"epoch\", # Her epoch sonunda değerlendir\n",
        "    load_best_model_at_end=True, # En iyi modeli yükle\n",
        "    metric_for_best_model=\"eval_loss\", # En iyi model için kullanılacak metrik\n",
        "    report_to=\"none\", # WandB vs. kullanmıyorsan \"none\"\n",
        ")\n",
        "\n",
        "# Trainer nesnesi\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset, # Test setini doğrulama için kullan\n",
        "    dataset_text_field=\"prompt\", # apply_chat_template sonrası \"text\" sütununu kullanıyoruz\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=os.cpu_count(),\n",
        "    packing=False, # Kısa diziler için True olabilir, ancak şu an False kalsın\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Eğitimi başlat\n",
        "print(\"\\n🚀 Eğitime Başlanıyor...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"✅ Eğitim Tamamlandı!\")"
      ],
      "metadata": {
        "id": "AEZKV_OLWBva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Modeli Kaydet (İsteğe Bağlı) ---\n",
        "# Eğitilmiş modeli daha sonra kullanmak üzere kaydet\n",
        "#trainer.save_model(\"/content/drive/My Drive/fine_tuned_flowchart_model\")\n",
        "#tokenizer.save_pretrained(\"/content/drive/My Drive/fine_tuned_flowchart_model\")\n",
        "#print(\"\\n✅ Model ve Tokenizer Drive'a kaydedildi.\")\n",
        "\n",
        "# --- 5. Test ve Çıkarım (Inference) ---\n",
        "print(\"\\n--- Test ve Çıkarım ---\")\n",
        "\n",
        "# Eğitimden sonra yeni bir model yüklemeye gerek yok, eğitilmiş 'model' objesini kullanıyoruz.\n",
        "\n",
        "# Örnek bir kullanıcı mesajı oluştur\n",
        "# Bu sistem mesajı, eğitim verinizdeki sistem mesajı ile aynı olmalı!\n",
        "system_message_for_inference = \"Görevin, verilen adımları analiz ederek bir akış diyagramı için hazırlamaktır. Her adım bir türle eşleştirilmelidir (Süreç, Koşul, Veritabanı, Mesaj Kutusu, E-posta Gönder, Başlat, Form Göster veya Web Servisi). Ayrıca, her adımın hangi adımdan sonra geldiğini de belirlemelisin. Eğer adım türü 'Koşul' ise, 'evet-sonraki' ve 'hayır-sonraki' yollarını açıkça belirtmelisin.\\n\\nÇıktın kesinlikle şu formatta olmalıdır:\\n\\nAdım [No]: [Tür] - '[Açıklama]' sonraki:[Sonraki Adım No]\\n\\nveya\\n\\nAdım [No]: [Tür] - '[Açıklama]' evet-sonraki:[Adım No], hayır-sonraki:[Adım No]\\n\\nYalnızca bu formatta ve açık, doğru çıktılar üret.\"\n",
        "\n",
        "user_input_example = \"1: Müşteri, sistem üzerinden sipariş oluşturur.\\n2: Sipariş bilgileri veritabanına kaydedilir.\\n3: Ürün stokta var mı? Evet ise ürün hazırlanır ve kargoya verilir. Hayır ise müşteriye stokta olmadığı bildirilir.\\n4: Ürün hazırlanır ve kargoya teslim edilir.\\n5: Hayır, stokta yok. Müşteriye ürünün stokta olmadığına dair e-posta gönderilir.\\n6: Sipariş müşteriye teslim edilir ve süreç tamamlanır.\"\n",
        "\n",
        "messages_for_inference = [\n",
        "    {\"role\": \"system\", \"content\": system_message_for_inference},\n",
        "    {\"role\": \"user\", \"content\": user_input_example}\n",
        "]\n",
        "\n",
        "# Tokenize et ve generation prompt'unu ekle\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages_for_inference,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True, # Llama-3'ün üretmeye başlaması için bu gerekli\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# TextStreamer ile çıktıyı akıt\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Model ile metin üret\n",
        "print(\"\\n🤖 Model Tahmini (Örnek):\")\n",
        "_ = model.generate(input_ids=input_ids,\n",
        "                     max_new_tokens=256, # Üretilecek maksimum token sayısı\n",
        "                     pad_token_id=tokenizer.eos_token_id,\n",
        "                     streamer=text_streamer)\n"
      ],
      "metadata": {
        "id": "55NdygtAWFYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Metrikleri Hesapla (Test Verisi Üzerinden) ---\n",
        "print(\"\\n--- Metrik Hesaplama ---\")\n",
        "\n",
        "# Gerekli kütüphaneleri yükle (Colab oturumunda bir kez çalıştırılır)\n",
        "# !pip install rouge_score\n",
        "# !pip install evaluate\n",
        "\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "# Metrikleri yükle\n",
        "bleu_metric = load(\"bleu\")\n",
        "rouge_metric = load(\"rouge\")\n",
        "\n",
        "true_outputs = []\n",
        "pred_outputs = []\n",
        "\n",
        "# Test setindeki raw veriyi kullan\n",
        "for item in tqdm(test_data_raw, desc=\"Generating predictions for test set\"):\n",
        "    # Her bir öğenin mesajlar listesinden kullanıcı girdisini ve asistan çıktısını al\n",
        "    user_msg = next((m[\"content\"] for m in item[\"messages\"] if m[\"role\"] == \"user\"), None)\n",
        "    expected_output = next((m[\"content\"] for m in item[\"messages\"] if m[\"role\"] == \"assistant\"), None)\n",
        "    system_msg = next((m[\"content\"] for m in item[\"messages\"] if m[\"role\"] == \"system\"), None) # Sisteminizi de alın\n",
        "\n",
        "    if user_msg and expected_output and system_msg:\n",
        "        inference_messages = [\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg}\n",
        "        ]\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            inference_messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad(): # Tahmin yaparken gradient hesaplamayı kapat\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256, # Üretilecek maksimum token sayısı\n",
        "                do_sample=False, # Daha deterministik çıktı için False\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Prompt kısmını atlayarak sadece modelin ürettiği cevabı decode et\n",
        "        # generate fonksiyonu tüm input_ids'i döndürür, bu yüzden prompt uzunluğunu çıkarırız\n",
        "        predicted_output = tokenizer.decode(generated_ids[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True).strip()\n",
        "\n",
        "        true_outputs.append(expected_output.strip())\n",
        "        pred_outputs.append(predicted_output.strip())\n",
        "\n",
        "# Eğer tahmin ve gerçek çıktı listeleri boş değilse metrikleri hesapla\n",
        "if len(true_outputs) > 0 and len(pred_outputs) > 0:\n",
        "    # ✅ Exact Match Accuracy\n",
        "    exact_matches = [int(p == t) for p, t in zip(pred_outputs, true_outputs)]\n",
        "    exact_accuracy = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "    # ✅ Relaxed Accuracy (küçük harfe çevir, boşlukları kaldır)\n",
        "    def relaxed_match(pred, ref):\n",
        "        return pred.lower().replace(\" \", \"\").replace(\"'\", \"\") == ref.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
        "\n",
        "    relaxed_matches = [int(relaxed_match(p, t)) for p, t in zip(pred_outputs, true_outputs)]\n",
        "    relaxed_accuracy = sum(relaxed_matches) / len(relaxed_matches)\n",
        "\n",
        "    # ✅ BLEU ve ROUGE hesapla\n",
        "    # BLEU tek referans için çift liste bekler\n",
        "    bleu_score = bleu_metric.compute(predictions=pred_outputs, references=[[ref] for ref in true_outputs])\n",
        "    rouge_score = rouge_metric.compute(predictions=pred_outputs, references=true_outputs)\n",
        "\n",
        "    # ✅ F1 hesaplama (basit token bazlı)\n",
        "    def compute_f1_token_based(pred, ref):\n",
        "        pred_tokens = pred.lower().split()\n",
        "        ref_tokens = ref.lower().split()\n",
        "\n",
        "        if not pred_tokens and not ref_tokens:\n",
        "            return 1.0 # Her ikisi de boşsa mükemmel eşleşme\n",
        "\n",
        "        common = len(set(pred_tokens) & set(ref_tokens))\n",
        "        if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
        "            return 0.0 # Birisi boşsa, diğerleri değilse eşleşme yok\n",
        "\n",
        "        precision = common / len(pred_tokens)\n",
        "        recall = common / len(ref_tokens)\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    f1_scores = [compute_f1_token_based(p, r) for p, r in zip(pred_outputs, true_outputs)]\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    # ✅ Sonuçları yazdır\n",
        "    print(f\"🔍 Ortalama Token Bazlı F1 Skoru: {avg_f1:.4f}\")\n",
        "    print(f\"✅ Exact Match Accuracy: {exact_accuracy:.4f}\")\n",
        "    print(f\"🎯 Relaxed Accuracy: {relaxed_accuracy:.4f}\")\n",
        "    print(f\"📘 BLEU: {round(bleu_score['bleu'], 4)}\")\n",
        "    print(\"📗 ROUGE:\", {k: round(v, 4) for k, v in rouge_score.items()})\n"
      ],
      "metadata": {
        "id": "aOzq2fesWNBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Örnek tahminleri göster\n",
        "    print(\"\\n--- Örnek Tahminler ve Gerçek Çıktılar ---\")\n",
        "    for i in range(min(15, len(test_data_raw))): # İlk 15 veya mevcut veri sayısı kadar göster\n",
        "        user_msg = next((m[\"content\"] for m in test_data_raw[i][\"messages\"] if m[\"role\"] == \"user\"), \"N/A\")\n",
        "        expected_output = next((m[\"content\"] for m in test_data_raw[i][\"messages\"] if m[\"role\"] == \"assistant\"), \"N/A\")\n",
        "\n",
        "        print(f\"🔹 Input: {user_msg}\")\n",
        "        print(f\"✅ Expected: {expected_output}\")\n",
        "        print(f\"🤖 Predicted: {pred_outputs[i]}\")\n",
        "        print(\"---\")\n",
        "else:\n",
        "    print(\"Metrik hesaplamak için yeterli test verisi bulunamadı veya tahminler boş.\")\n"
      ],
      "metadata": {
        "id": "GZ8YNfRpWSNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def parse_flowchart_step(step_text):\n",
        "    \"\"\"\n",
        "    Verilen akış diyagramı adım metnini parse ederek yapılandırılmış bir sözlük döndürür.\n",
        "    Format: Adım [No]: [Tür] - '[Açıklama]' next:[Sonraki Adım No]\n",
        "    veya: Adım [No]: [Tür] - '[Açıklama]' yes-next:[Adım No], no-next:[Adım No]\n",
        "    \"\"\"\n",
        "    parsed_data = {\n",
        "        \"num\": None,\n",
        "        \"type\": None,\n",
        "        \"description\": None,\n",
        "        \"next\": None,\n",
        "        \"yes_next\": None,\n",
        "        \"no_next\": None,\n",
        "    }\n",
        "\n",
        "    # Adım numarasını ve türü yakala\n",
        "    match = re.match(r\"Adım (\\d+): (Süreç|Koşul|Veritabanı|Mesaj Kutusu|E-posta Gönder|Başlat|Form Göster|Web Servisi) - '(.*?)'\", step_text)\n",
        "    if match:\n",
        "        parsed_data[\"num\"] = match.group(1)\n",
        "        parsed_data[\"type\"] = match.group(2)\n",
        "        parsed_data[\"description\"] = match.group(3).strip()\n",
        "\n",
        "        # Sonraki adım bağlantılarını yakala\n",
        "        remaining_text = step_text[match.end():].strip()\n",
        "        if remaining_text.startswith(\",\"):\n",
        "            remaining_text = remaining_text[1:].strip() # Baştaki virgülü kaldır\n",
        "\n",
        "        # Koşul durumu\n",
        "        if \"yes-next:\" in remaining_text and \"no-next:\" in remaining_text:\n",
        "            yes_match = re.search(r\"yes-next:(\\d+)\", remaining_text)\n",
        "            no_match = re.search(r\"no-next:(\\d+)\", remaining_text)\n",
        "            if yes_match and no_match:\n",
        "                parsed_data[\"yes_next\"] = yes_match.group(1)\n",
        "                parsed_data[\"no_next\"] = no_match.group(1)\n",
        "        # Tekli next durumu\n",
        "        elif \"next:\" in remaining_text:\n",
        "            next_match = re.search(r\"next:(\\d+|None)\", remaining_text)\n",
        "            if next_match:\n",
        "                parsed_data[\"next\"] = next_match.group(1)\n",
        "    return parsed_data\n",
        "\n",
        "def calculate_component_metrics(true_outputs, pred_outputs):\n",
        "    \"\"\"\n",
        "    Her bir bileşen (adım numarası, tür, sonraki adım) için doğruluk, F1, Precision, Recall hesaplar.\n",
        "    \"\"\"\n",
        "    num_matches = []\n",
        "    type_matches = []\n",
        "    next_matches = [] # next, yes_next, no_next için genel eşleşme\n",
        "\n",
        "    # Açıklama metinleri için ayrı listeler (BLEU/ROUGE zaten bunları kullanıyor)\n",
        "    true_descriptions = []\n",
        "    pred_descriptions = []\n",
        "\n",
        "    # Token tabanlı F1 için\n",
        "    all_true_tokens = []\n",
        "    all_pred_tokens = []\n",
        "\n",
        "    for true_text, pred_text in zip(true_outputs, pred_outputs):\n",
        "        # Her bir çok satırlı çıktıyı ayrı adımlara böl\n",
        "        true_steps = true_text.split('\\n')\n",
        "        pred_steps = pred_text.split('\\n')\n",
        "\n",
        "        # Adım numaralarına göre eşleştirme yapabilmek için sözlüklere çevir\n",
        "        true_parsed_map = {p[\"num\"]: p for p in [parse_flowchart_step(s) for s in true_steps] if p[\"num\"]}\n",
        "        pred_parsed_map = {p[\"num\"]: p for p in [parse_flowchart_step(s) for s in pred_steps] if p[\"num\"]}\n",
        "\n",
        "        for num, true_step_data in true_parsed_map.items():\n",
        "            pred_step_data = pred_parsed_map.get(num)\n",
        "\n",
        "            # 1. Adım Numarası Doğruluğu (Bu zaten eşleşme anahtarımız, ancak yine de kontrol edelim)\n",
        "            num_matches.append(1 if pred_step_data and pred_step_data[\"num\"] == true_step_data[\"num\"] else 0)\n",
        "\n",
        "            if pred_step_data: # Eğer bu adım için bir tahmin varsa\n",
        "                # 2. Adım Türü Doğruluğu\n",
        "                type_matches.append(1 if pred_step_data[\"type\"] == true_step_data[\"type\"] else 0)\n",
        "\n",
        "                # 3. Sonraki Adım Bağlantı Doğruluğu\n",
        "                is_next_match = 0\n",
        "                if true_step_data[\"type\"] == \"Koşul\":\n",
        "                    if (pred_step_data[\"yes_next\"] == true_step_data[\"yes_next\"] and\n",
        "                        pred_step_data[\"no_next\"] == true_step_data[\"no_next\"]):\n",
        "                        is_next_match = 1\n",
        "                else: # Süreç, Veritabanı vb.\n",
        "                    if pred_step_data[\"next\"] == true_step_data[\"next\"]:\n",
        "                        is_next_match = 1\n",
        "                next_matches.append(is_next_match)\n",
        "\n",
        "                # 4. Açıklama Metni İçin Liste Hazırlığı (Zaten BLEU/ROUGE'da kullanılıyor)\n",
        "                true_descriptions.append(true_step_data[\"description\"])\n",
        "                pred_descriptions.append(pred_step_data[\"description\"])\n",
        "\n",
        "                # 5. Token Tabanlı F1 için token listeleri\n",
        "                all_true_tokens.extend(str(true_step_data.get(\"type\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"description\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"next\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"yes_next\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"no_next\", \"\")).lower().split())\n",
        "                all_pred_tokens.extend(str(pred_step_data.get(\"type\", \"\")).lower().split() + \\\n",
        "                                      str(pred_step_data.get(\"description\", \"\")).lower().split() + \\\n",
        "                                      str(pred_step_data.get(\"next\", \"\")).lower().split() + \\\n",
        "                                      str(pred_step_data.get(\"yes_next\", \"\")).lower().split() + \\\n",
        "                                      str(pred_step_data.get(\"no_next\", \"\")).lower().split())\n",
        "            else: # Eğer tahmin edilen adım yoksa, hepsi yanlış sayılır\n",
        "                type_matches.append(0)\n",
        "                next_matches.append(0)\n",
        "                # Açıklama ve token listelerine boş ekle (ya da ignore edebiliriz)\n",
        "                true_descriptions.append(true_step_data[\"description\"])\n",
        "                pred_descriptions.append(\"\") # Tahmin yoksa boş açıklama\n",
        "\n",
        "                all_true_tokens.extend(str(true_step_data.get(\"type\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"description\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"next\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"yes_next\", \"\")).lower().split() + \\\n",
        "                                      str(true_step_data.get(\"no_next\", \"\")).lower().split())\n",
        "                # Pred tokenları boş kalır\n",
        "\n",
        "    metrics = {}\n",
        "    if num_matches:\n",
        "        metrics[\"step_num_accuracy\"] = np.mean(num_matches)\n",
        "    if type_matches:\n",
        "        metrics[\"step_type_accuracy\"] = np.mean(type_matches)\n",
        "    if next_matches:\n",
        "        metrics[\"next_link_accuracy\"] = np.mean(next_matches)\n",
        "\n",
        "    # Tüm tokenların üzerinden Precision, Recall, F1 hesaplama\n",
        "    if all_true_tokens and all_pred_tokens:\n",
        "        # sklearn'ın precision_recall_fscore_support'u kullanmak için etiketleme yapmalıyız\n",
        "        # Basit bir token tabanlı F1 için Counter kullanmak daha kolay olabilir\n",
        "        common_tokens = len(list((Counter(all_pred_tokens) & Counter(all_true_tokens)).elements()))\n",
        "\n",
        "        if len(all_pred_tokens) == 0:\n",
        "            precision = 0.0\n",
        "        else:\n",
        "            precision = common_tokens / len(all_pred_tokens)\n",
        "\n",
        "        if len(all_true_tokens) == 0:\n",
        "            recall = 0.0\n",
        "        else:\n",
        "            recall = common_tokens / len(all_true_tokens)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        metrics[\"token_precision\"] = precision\n",
        "        metrics[\"token_recall\"] = recall\n",
        "        metrics[\"token_f1\"] = f1\n",
        "\n",
        "    return metrics, true_descriptions, pred_descriptions\n",
        "\n",
        "# Not: BLEU ve ROUGE hesaplamaları için true_descriptions ve pred_descriptions kullanılacaktır.\n",
        "# Kodunuzda bu zaten mevcuttur."
      ],
      "metadata": {
        "id": "sJPEFZnyX_5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0d3be00"
      },
      "source": [
        "# Install necessary libraries for SFTTrainer\n",
        "!pip install trl accelerate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}